{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT TEXT MINING - POSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/text.txt\n",
      "does path exist? -> True\n"
     ]
    }
   ],
   "source": [
    "cur_dir = Path().resolve() # this should provide you with the folder in which this notebook is placed\n",
    "path_to_file = Path.joinpath(cur_dir, '/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/text.txt')\n",
    "print(path_to_file)\n",
    "print('does path exist? ->', Path.exists(path_to_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output from the code cell above states that **does path exist? -> False**, please check that the file **Lab1-apple-samsung-example.txt** is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 955\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "\n",
    "print('number of characters', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress SSL warnings\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Make a request with SSL verification disabled\n",
    "response = requests.get('https://google.com', verify=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence = []\n",
    "for sentence_nltk in sentences_nltk:\n",
    "    sent_tokens = word_tokenize(sentence_nltk)\n",
    "    tokens_per_sentence.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use lists to keep track of the output of the NLP tasks. We can hence inspect the output for each task using the index of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift.\n",
      "TOKENS ['I', 'would', \"n't\", 'be', 'caught', 'dead', 'watching', 'the', 'NFL', 'if', 'it', 'were', \"n't\", 'for', 'Taylor', 'Swift', '.']\n"
     ]
    }
   ],
   "source": [
    "sent_id = 0\n",
    "print('SENTENCE', sentences_nltk[sent_id])\n",
    "print('TOKENS', tokens_per_sentence[sent_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1a: Part-of-speech (POS) tagging\n",
    "Use `nltk.pos_tag` to perform part-of-speech tagging on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), ('be', 'VB'), ('caught', 'VBN'), ('dead', 'JJ'), ('watching', 'VBG'), ('the', 'DT'), ('NFL', 'NNP'), ('if', 'IN'), ('it', 'PRP'), ('were', 'VBD'), (\"n't\", 'RB'), ('for', 'IN'), ('Taylor', 'NNP'), ('Swift', 'NNP'), ('.', '.')]\n",
      "[('Chris', 'NNP'), (\"O'Donnell\", 'NNP'), ('stated', 'VBD'), ('that', 'IN'), ('while', 'IN'), ('filming', 'VBG'), ('for', 'IN'), ('this', 'DT'), ('movie', 'NN'), (',', ','), ('he', 'PRP'), ('felt', 'VBD'), ('like', 'IN'), ('he', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('Toys', 'NNP'), ('``', '``'), ('R', 'NNP'), (\"''\", \"''\"), ('Us', 'NNP'), ('commercial', 'JJ'), ('.', '.')]\n",
      "[('The', 'DT'), ('whole', 'JJ'), ('game', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('rollercoaster', 'JJ'), ('ride', 'NN'), (',', ','), ('but', 'CC'), ('Los', 'NNP'), ('Angeles', 'NNP'), ('Lakers', 'NNP'), ('ultimately', 'RB'), ('persevered', 'VBD'), ('and', 'CC'), ('won', 'VBD'), ('!', '.')]\n",
      "[('Zendaya', 'NNP'), ('slayed', 'VBD'), ('in', 'IN'), ('Dune', 'NNP'), ('2', 'CD'), (',', ','), ('as', 'IN'), ('she', 'PRP'), ('does', 'VBZ'), ('in', 'IN'), ('all', 'DT'), ('her', 'PRP$'), ('movies', 'NNS'), ('.', '.')]\n",
      "[('While', 'IN'), ('my', 'PRP$'), ('favorite', 'JJ'), ('player', 'NN'), ('was', 'VBD'), ('playing', 'VBG'), ('this', 'DT'), ('match', 'NN'), ('and', 'CC'), ('started', 'VBD'), ('off', 'RP'), ('strongggg', 'NN'), (',', ','), ('it', 'PRP'), ('went', 'VBD'), ('downhill', 'NN'), ('after', 'IN'), ('Messi', 'NNP'), (\"'s\", 'POS'), ('injyry', 'NN'), ('midgame', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('uncle', 'NN'), (\"'s\", 'POS'), ('brother', 'NN'), (\"'s\", 'POS'), ('neighbor', 'NN'), (\"'s\", 'POS'), ('cat', 'NN'), (\"'s\", 'POS'), ('veterinarian', 'JJ'), ('David', 'NNP'), ('reads', 'VBZ'), ('the', 'DT'), ('communist', 'NN'), ('manifesto', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('spare', 'JJ'), ('time', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('The', 'DT'), ('Great', 'NNP'), ('Gatsby', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('novell', 'NN'), ('ever', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('about', 'RB'), ('to', 'TO'), ('throw', 'VB'), ('hands', 'NNS'), ('.', '.')]\n",
      "[('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('look', 'VB'), ('away', 'RB'), ('from', 'IN'), ('this', 'DT'), ('train', 'NN'), ('wrck', 'NN'), ('of', 'IN'), ('a', 'DT'), ('movie', 'NN'), (',', ','), ('on', 'IN'), ('February', 'NNP'), ('14th', 'CD'), ('of', 'IN'), ('all', 'DT'), ('days', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('film', 'NN'), ('Everything', 'NNP'), ('Everywhere', 'NNP'), ('All', 'NNP'), ('At', 'IN'), ('Once', 'NNP'), ('follows', 'VBZ'), ('Evelyn', 'NNP'), ('Wang', 'NNP'), (',', ','), ('a', 'DT'), ('woman', 'NN'), ('drowning', 'VBG'), ('under', 'IN'), ('the', 'DT'), ('stress', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('failing', 'JJ'), ('laundromat', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('just', 'RB'), ('finished', 'VBD'), ('reading', 'VBG'), ('pride', 'NN'), ('and', 'CC'), ('prejudice', 'NN'), ('which', 'WDT'), ('had', 'VBD'), ('me', 'PRP'), ('HOOOKED', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('beginning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags_per_sentence = []\n",
    "for tokens in tokens_per_sentence:\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    pos_tags_per_sentence.append(pos_tags)\n",
    "    print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), ('be', 'VB'), ('caught', 'VBN'), ('dead', 'JJ'), ('watching', 'VBG'), ('the', 'DT'), ('NFL', 'NNP'), ('if', 'IN'), ('it', 'PRP'), ('were', 'VBD'), (\"n't\", 'RB'), ('for', 'IN'), ('Taylor', 'NNP'), ('Swift', 'NNP'), ('.', '.')], [('Chris', 'NNP'), (\"O'Donnell\", 'NNP'), ('stated', 'VBD'), ('that', 'IN'), ('while', 'IN'), ('filming', 'VBG'), ('for', 'IN'), ('this', 'DT'), ('movie', 'NN'), (',', ','), ('he', 'PRP'), ('felt', 'VBD'), ('like', 'IN'), ('he', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('Toys', 'NNP'), ('``', '``'), ('R', 'NNP'), (\"''\", \"''\"), ('Us', 'NNP'), ('commercial', 'JJ'), ('.', '.')], [('The', 'DT'), ('whole', 'JJ'), ('game', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('rollercoaster', 'JJ'), ('ride', 'NN'), (',', ','), ('but', 'CC'), ('Los', 'NNP'), ('Angeles', 'NNP'), ('Lakers', 'NNP'), ('ultimately', 'RB'), ('persevered', 'VBD'), ('and', 'CC'), ('won', 'VBD'), ('!', '.')], [('Zendaya', 'NNP'), ('slayed', 'VBD'), ('in', 'IN'), ('Dune', 'NNP'), ('2', 'CD'), (',', ','), ('as', 'IN'), ('she', 'PRP'), ('does', 'VBZ'), ('in', 'IN'), ('all', 'DT'), ('her', 'PRP$'), ('movies', 'NNS'), ('.', '.')], [('While', 'IN'), ('my', 'PRP$'), ('favorite', 'JJ'), ('player', 'NN'), ('was', 'VBD'), ('playing', 'VBG'), ('this', 'DT'), ('match', 'NN'), ('and', 'CC'), ('started', 'VBD'), ('off', 'RP'), ('strongggg', 'NN'), (',', ','), ('it', 'PRP'), ('went', 'VBD'), ('downhill', 'NN'), ('after', 'IN'), ('Messi', 'NNP'), (\"'s\", 'POS'), ('injyry', 'NN'), ('midgame', 'NN'), ('.', '.')], [('My', 'PRP$'), ('uncle', 'NN'), (\"'s\", 'POS'), ('brother', 'NN'), (\"'s\", 'POS'), ('neighbor', 'NN'), (\"'s\", 'POS'), ('cat', 'NN'), (\"'s\", 'POS'), ('veterinarian', 'JJ'), ('David', 'NNP'), ('reads', 'VBZ'), ('the', 'DT'), ('communist', 'NN'), ('manifesto', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('spare', 'JJ'), ('time', 'NN'), ('.', '.')], [('He', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('The', 'DT'), ('Great', 'NNP'), ('Gatsby', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('novell', 'NN'), ('ever', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('about', 'RB'), ('to', 'TO'), ('throw', 'VB'), ('hands', 'NNS'), ('.', '.')], [('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('look', 'VB'), ('away', 'RB'), ('from', 'IN'), ('this', 'DT'), ('train', 'NN'), ('wrck', 'NN'), ('of', 'IN'), ('a', 'DT'), ('movie', 'NN'), (',', ','), ('on', 'IN'), ('February', 'NNP'), ('14th', 'CD'), ('of', 'IN'), ('all', 'DT'), ('days', 'NNS'), ('.', '.')], [('The', 'DT'), ('film', 'NN'), ('Everything', 'NNP'), ('Everywhere', 'NNP'), ('All', 'NNP'), ('At', 'IN'), ('Once', 'NNP'), ('follows', 'VBZ'), ('Evelyn', 'NNP'), ('Wang', 'NNP'), (',', ','), ('a', 'DT'), ('woman', 'NN'), ('drowning', 'VBG'), ('under', 'IN'), ('the', 'DT'), ('stress', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('failing', 'JJ'), ('laundromat', 'NN'), ('.', '.')], [('I', 'PRP'), ('just', 'RB'), ('finished', 'VBD'), ('reading', 'VBG'), ('pride', 'NN'), ('and', 'CC'), ('prejudice', 'NN'), ('which', 'WDT'), ('had', 'VBD'), ('me', 'PRP'), ('HOOOKED', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('beginning', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1b: Named Entity Recognition (NER)\n",
    "Use `nltk.chunk.ne_chunk` to perform Named Entity Recognition (NER) on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  would/MD\n",
      "  n't/RB\n",
      "  be/VB\n",
      "  caught/VBN\n",
      "  dead/JJ\n",
      "  watching/VBG\n",
      "  the/DT\n",
      "  (ORGANIZATION NFL/NNP)\n",
      "  if/IN\n",
      "  it/PRP\n",
      "  were/VBD\n",
      "  n't/RB\n",
      "  for/IN\n",
      "  (PERSON Taylor/NNP Swift/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Chris/NNP)\n",
      "  O'Donnell/NNP\n",
      "  stated/VBD\n",
      "  that/IN\n",
      "  while/IN\n",
      "  filming/VBG\n",
      "  for/IN\n",
      "  this/DT\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  felt/VBD\n",
      "  like/IN\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  in/IN\n",
      "  a/DT\n",
      "  Toys/NNP\n",
      "  ``/``\n",
      "  R/NNP\n",
      "  ''/''\n",
      "  (GPE Us/NNP)\n",
      "  commercial/JJ\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  whole/JJ\n",
      "  game/NN\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  rollercoaster/JJ\n",
      "  ride/NN\n",
      "  ,/,\n",
      "  but/CC\n",
      "  (GPE Los/NNP Angeles/NNP)\n",
      "  Lakers/NNP\n",
      "  ultimately/RB\n",
      "  persevered/VBD\n",
      "  and/CC\n",
      "  won/VBD\n",
      "  !/.)\n",
      "(S\n",
      "  (PERSON Zendaya/NNP)\n",
      "  slayed/VBD\n",
      "  in/IN\n",
      "  Dune/NNP\n",
      "  2/CD\n",
      "  ,/,\n",
      "  as/IN\n",
      "  she/PRP\n",
      "  does/VBZ\n",
      "  in/IN\n",
      "  all/DT\n",
      "  her/PRP$\n",
      "  movies/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  While/IN\n",
      "  my/PRP$\n",
      "  favorite/JJ\n",
      "  player/NN\n",
      "  was/VBD\n",
      "  playing/VBG\n",
      "  this/DT\n",
      "  match/NN\n",
      "  and/CC\n",
      "  started/VBD\n",
      "  off/RP\n",
      "  strongggg/NN\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  went/VBD\n",
      "  downhill/NN\n",
      "  after/IN\n",
      "  (PERSON Messi/NNP)\n",
      "  's/POS\n",
      "  injyry/NN\n",
      "  midgame/NN\n",
      "  ./.)\n",
      "(S\n",
      "  My/PRP$\n",
      "  uncle/NN\n",
      "  's/POS\n",
      "  brother/NN\n",
      "  's/POS\n",
      "  neighbor/NN\n",
      "  's/POS\n",
      "  cat/NN\n",
      "  's/POS\n",
      "  veterinarian/JJ\n",
      "  (PERSON David/NNP)\n",
      "  reads/VBZ\n",
      "  the/DT\n",
      "  communist/NN\n",
      "  manifesto/NN\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  spare/JJ\n",
      "  time/NN\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  said/VBD\n",
      "  that/IN\n",
      "  The/DT\n",
      "  (ORGANIZATION Great/NNP Gatsby/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  novell/NN\n",
      "  ever/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  was/VBD\n",
      "  about/RB\n",
      "  to/TO\n",
      "  throw/VB\n",
      "  hands/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  could/MD\n",
      "  not/RB\n",
      "  look/VB\n",
      "  away/RB\n",
      "  from/IN\n",
      "  this/DT\n",
      "  train/NN\n",
      "  wrck/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  on/IN\n",
      "  February/NNP\n",
      "  14th/CD\n",
      "  of/IN\n",
      "  all/DT\n",
      "  days/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  film/NN\n",
      "  Everything/NNP\n",
      "  Everywhere/NNP\n",
      "  All/NNP\n",
      "  At/IN\n",
      "  (ORGANIZATION Once/NNP)\n",
      "  follows/VBZ\n",
      "  (PERSON Evelyn/NNP Wang/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  woman/NN\n",
      "  drowning/VBG\n",
      "  under/IN\n",
      "  the/DT\n",
      "  stress/NN\n",
      "  of/IN\n",
      "  her/PRP$\n",
      "  family/NN\n",
      "  's/POS\n",
      "  failing/JJ\n",
      "  laundromat/NN\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  just/RB\n",
      "  finished/VBD\n",
      "  reading/VBG\n",
      "  pride/NN\n",
      "  and/CC\n",
      "  prejudice/NN\n",
      "  which/WDT\n",
      "  had/VBD\n",
      "  me/PRP\n",
      "  HOOOKED/NNP\n",
      "  from/IN\n",
      "  the/DT\n",
      "  beginning/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner_per_sentence = []\n",
    "for tokens in tokens_per_sentence:\n",
    "    ner_tags = nltk.chunk.ne_chunk(nltk.pos_tag(tokens))\n",
    "    ner_per_sentence.append(ner_tags)\n",
    "    print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), ('be', 'VB'), ('caught', 'VBN'), ('dead', 'JJ'), ('watching', 'VBG'), ('the', 'DT'), Tree('ORGANIZATION', [('NFL', 'NNP')]), ('if', 'IN'), ('it', 'PRP'), ('were', 'VBD'), (\"n't\", 'RB'), ('for', 'IN'), Tree('PERSON', [('Taylor', 'NNP'), ('Swift', 'NNP')]), ('.', '.')]), Tree('S', [Tree('PERSON', [('Chris', 'NNP')]), (\"O'Donnell\", 'NNP'), ('stated', 'VBD'), ('that', 'IN'), ('while', 'IN'), ('filming', 'VBG'), ('for', 'IN'), ('this', 'DT'), ('movie', 'NN'), (',', ','), ('he', 'PRP'), ('felt', 'VBD'), ('like', 'IN'), ('he', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('Toys', 'NNP'), ('``', '``'), ('R', 'NNP'), (\"''\", \"''\"), Tree('GPE', [('Us', 'NNP')]), ('commercial', 'JJ'), ('.', '.')]), Tree('S', [('The', 'DT'), ('whole', 'JJ'), ('game', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('rollercoaster', 'JJ'), ('ride', 'NN'), (',', ','), ('but', 'CC'), Tree('GPE', [('Los', 'NNP'), ('Angeles', 'NNP')]), ('Lakers', 'NNP'), ('ultimately', 'RB'), ('persevered', 'VBD'), ('and', 'CC'), ('won', 'VBD'), ('!', '.')]), Tree('S', [Tree('PERSON', [('Zendaya', 'NNP')]), ('slayed', 'VBD'), ('in', 'IN'), ('Dune', 'NNP'), ('2', 'CD'), (',', ','), ('as', 'IN'), ('she', 'PRP'), ('does', 'VBZ'), ('in', 'IN'), ('all', 'DT'), ('her', 'PRP$'), ('movies', 'NNS'), ('.', '.')]), Tree('S', [('While', 'IN'), ('my', 'PRP$'), ('favorite', 'JJ'), ('player', 'NN'), ('was', 'VBD'), ('playing', 'VBG'), ('this', 'DT'), ('match', 'NN'), ('and', 'CC'), ('started', 'VBD'), ('off', 'RP'), ('strongggg', 'NN'), (',', ','), ('it', 'PRP'), ('went', 'VBD'), ('downhill', 'NN'), ('after', 'IN'), Tree('PERSON', [('Messi', 'NNP')]), (\"'s\", 'POS'), ('injyry', 'NN'), ('midgame', 'NN'), ('.', '.')]), Tree('S', [('My', 'PRP$'), ('uncle', 'NN'), (\"'s\", 'POS'), ('brother', 'NN'), (\"'s\", 'POS'), ('neighbor', 'NN'), (\"'s\", 'POS'), ('cat', 'NN'), (\"'s\", 'POS'), ('veterinarian', 'JJ'), Tree('PERSON', [('David', 'NNP')]), ('reads', 'VBZ'), ('the', 'DT'), ('communist', 'NN'), ('manifesto', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('spare', 'JJ'), ('time', 'NN'), ('.', '.')]), Tree('S', [('He', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('The', 'DT'), Tree('ORGANIZATION', [('Great', 'NNP'), ('Gatsby', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('novell', 'NN'), ('ever', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('about', 'RB'), ('to', 'TO'), ('throw', 'VB'), ('hands', 'NNS'), ('.', '.')]), Tree('S', [('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('look', 'VB'), ('away', 'RB'), ('from', 'IN'), ('this', 'DT'), ('train', 'NN'), ('wrck', 'NN'), ('of', 'IN'), ('a', 'DT'), ('movie', 'NN'), (',', ','), ('on', 'IN'), ('February', 'NNP'), ('14th', 'CD'), ('of', 'IN'), ('all', 'DT'), ('days', 'NNS'), ('.', '.')]), Tree('S', [('The', 'DT'), ('film', 'NN'), ('Everything', 'NNP'), ('Everywhere', 'NNP'), ('All', 'NNP'), ('At', 'IN'), Tree('ORGANIZATION', [('Once', 'NNP')]), ('follows', 'VBZ'), Tree('PERSON', [('Evelyn', 'NNP'), ('Wang', 'NNP')]), (',', ','), ('a', 'DT'), ('woman', 'NN'), ('drowning', 'VBG'), ('under', 'IN'), ('the', 'DT'), ('stress', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('failing', 'JJ'), ('laundromat', 'NN'), ('.', '.')]), Tree('S', [('I', 'PRP'), ('just', 'RB'), ('finished', 'VBD'), ('reading', 'VBG'), ('pride', 'NN'), ('and', 'CC'), ('prejudice', 'NN'), ('which', 'WDT'), ('had', 'VBD'), ('me', 'PRP'), ('HOOOKED', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('beginning', 'NN'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "print(ner_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 1c: Constituency parsing\n",
    "Use the `nltk.RegexpParser` to perform constituency parsing on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  would/MD\n",
      "  n't/RB\n",
      "  (VP (V be/VB))\n",
      "  (VP (V caught/VBN) (NP dead/JJ))\n",
      "  (VP (V watching/VBG) (NP the/DT))\n",
      "  NFL/NNP\n",
      "  (P if/IN)\n",
      "  it/PRP\n",
      "  (VP (V were/VBD))\n",
      "  n't/RB\n",
      "  (P for/IN)\n",
      "  Taylor/NNP\n",
      "  Swift/NNP\n",
      "  ./.)\n",
      "(S\n",
      "  Chris/NNP\n",
      "  O'Donnell/NNP\n",
      "  (VP (V stated/VBD))\n",
      "  (P that/IN)\n",
      "  (P while/IN)\n",
      "  (VP (V filming/VBG) (PP (P for/IN) (NP this/DT movie/NN)))\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP (V felt/VBD))\n",
      "  (P like/IN)\n",
      "  he/PRP\n",
      "  (VP (V was/VBD) (PP (P in/IN) (NP a/DT)))\n",
      "  Toys/NNP\n",
      "  ``/``\n",
      "  R/NNP\n",
      "  ''/''\n",
      "  Us/NNP\n",
      "  (NP commercial/JJ)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT whole/JJ game/NN)\n",
      "  (VP (V was/VBD) (NP a/DT rollercoaster/JJ ride/NN))\n",
      "  ,/,\n",
      "  but/CC\n",
      "  Los/NNP\n",
      "  Angeles/NNP\n",
      "  Lakers/NNP\n",
      "  ultimately/RB\n",
      "  (VP (V persevered/VBD))\n",
      "  and/CC\n",
      "  (VP (V won/VBD))\n",
      "  !/.)\n",
      "(S\n",
      "  Zendaya/NNP\n",
      "  (VP (V slayed/VBD))\n",
      "  (P in/IN)\n",
      "  Dune/NNP\n",
      "  2/CD\n",
      "  ,/,\n",
      "  (P as/IN)\n",
      "  she/PRP\n",
      "  (VP (V does/VBZ) (PP (P in/IN) (NP all/DT)))\n",
      "  her/PRP$\n",
      "  movies/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (P While/IN)\n",
      "  my/PRP$\n",
      "  (NP favorite/JJ player/NN)\n",
      "  (VP (V was/VBD))\n",
      "  (VP (V playing/VBG) (NP this/DT match/NN))\n",
      "  and/CC\n",
      "  (VP (V started/VBD))\n",
      "  off/RP\n",
      "  (NP strongggg/NN)\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  (VP (V went/VBD) (NP downhill/NN))\n",
      "  (P after/IN)\n",
      "  Messi/NNP\n",
      "  's/POS\n",
      "  (NP injyry/NN midgame/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  My/PRP$\n",
      "  (NP uncle/NN)\n",
      "  's/POS\n",
      "  (NP brother/NN)\n",
      "  's/POS\n",
      "  (NP neighbor/NN)\n",
      "  's/POS\n",
      "  (NP cat/NN)\n",
      "  's/POS\n",
      "  (NP veterinarian/JJ)\n",
      "  David/NNP\n",
      "  (VP (V reads/VBZ) (NP the/DT communist/NN manifesto/NN))\n",
      "  (P in/IN)\n",
      "  his/PRP$\n",
      "  (NP spare/JJ time/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  (VP (V said/VBD) (PP (P that/IN) (NP The/DT)))\n",
      "  Great/NNP\n",
      "  Gatsby/NNP\n",
      "  (VP (V is/VBZ) (NP the/DT))\n",
      "  best/JJS\n",
      "  (NP novell/NN)\n",
      "  ever/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (VP (V was/VBD))\n",
      "  about/RB\n",
      "  to/TO\n",
      "  (VP (V throw/VB))\n",
      "  hands/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  could/MD\n",
      "  not/RB\n",
      "  (VP (V look/VB))\n",
      "  away/RB\n",
      "  (PP (P from/IN) (NP this/DT train/NN wrck/NN))\n",
      "  (PP (P of/IN) (NP a/DT movie/NN))\n",
      "  ,/,\n",
      "  (P on/IN)\n",
      "  February/NNP\n",
      "  14th/CD\n",
      "  (PP (P of/IN) (NP all/DT))\n",
      "  days/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT film/NN)\n",
      "  Everything/NNP\n",
      "  Everywhere/NNP\n",
      "  All/NNP\n",
      "  (P At/IN)\n",
      "  Once/NNP\n",
      "  (VP (V follows/VBZ))\n",
      "  Evelyn/NNP\n",
      "  Wang/NNP\n",
      "  ,/,\n",
      "  (NP a/DT woman/NN)\n",
      "  (VP (V drowning/VBG) (PP (P under/IN) (NP the/DT stress/NN)))\n",
      "  (P of/IN)\n",
      "  her/PRP$\n",
      "  (NP family/NN)\n",
      "  's/POS\n",
      "  (NP failing/JJ laundromat/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  just/RB\n",
      "  (VP (V finished/VBD))\n",
      "  (VP (V reading/VBG) (NP pride/NN))\n",
      "  and/CC\n",
      "  (NP prejudice/NN)\n",
      "  which/WDT\n",
      "  (VP (V had/VBD))\n",
      "  me/PRP\n",
      "  HOOOKED/NNP\n",
      "  (PP (P from/IN) (NP the/DT beginning/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "constituency_output_per_sentence = []\n",
    "for tokens in tokens_per_sentence:\n",
    "    constituency_output = constituent_parser.parse(nltk.pos_tag(tokens))\n",
    "    constituency_output_per_sentence.append(constituency_output)\n",
    "    print(constituency_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), Tree('VP', [Tree('V', [('be', 'VB')])]), Tree('VP', [Tree('V', [('caught', 'VBN')]), Tree('NP', [('dead', 'JJ')])]), Tree('VP', [Tree('V', [('watching', 'VBG')]), Tree('NP', [('the', 'DT')])]), ('NFL', 'NNP'), Tree('P', [('if', 'IN')]), ('it', 'PRP'), Tree('VP', [Tree('V', [('were', 'VBD')])]), (\"n't\", 'RB'), Tree('P', [('for', 'IN')]), ('Taylor', 'NNP'), ('Swift', 'NNP'), ('.', '.')]), Tree('S', [('Chris', 'NNP'), (\"O'Donnell\", 'NNP'), Tree('VP', [Tree('V', [('stated', 'VBD')])]), Tree('P', [('that', 'IN')]), Tree('P', [('while', 'IN')]), Tree('VP', [Tree('V', [('filming', 'VBG')]), Tree('PP', [Tree('P', [('for', 'IN')]), Tree('NP', [('this', 'DT'), ('movie', 'NN')])])]), (',', ','), ('he', 'PRP'), Tree('VP', [Tree('V', [('felt', 'VBD')])]), Tree('P', [('like', 'IN')]), ('he', 'PRP'), Tree('VP', [Tree('V', [('was', 'VBD')]), Tree('PP', [Tree('P', [('in', 'IN')]), Tree('NP', [('a', 'DT')])])]), ('Toys', 'NNP'), ('``', '``'), ('R', 'NNP'), (\"''\", \"''\"), ('Us', 'NNP'), Tree('NP', [('commercial', 'JJ')]), ('.', '.')]), Tree('S', [Tree('NP', [('The', 'DT'), ('whole', 'JJ'), ('game', 'NN')]), Tree('VP', [Tree('V', [('was', 'VBD')]), Tree('NP', [('a', 'DT'), ('rollercoaster', 'JJ'), ('ride', 'NN')])]), (',', ','), ('but', 'CC'), ('Los', 'NNP'), ('Angeles', 'NNP'), ('Lakers', 'NNP'), ('ultimately', 'RB'), Tree('VP', [Tree('V', [('persevered', 'VBD')])]), ('and', 'CC'), Tree('VP', [Tree('V', [('won', 'VBD')])]), ('!', '.')]), Tree('S', [('Zendaya', 'NNP'), Tree('VP', [Tree('V', [('slayed', 'VBD')])]), Tree('P', [('in', 'IN')]), ('Dune', 'NNP'), ('2', 'CD'), (',', ','), Tree('P', [('as', 'IN')]), ('she', 'PRP'), Tree('VP', [Tree('V', [('does', 'VBZ')]), Tree('PP', [Tree('P', [('in', 'IN')]), Tree('NP', [('all', 'DT')])])]), ('her', 'PRP$'), ('movies', 'NNS'), ('.', '.')]), Tree('S', [Tree('P', [('While', 'IN')]), ('my', 'PRP$'), Tree('NP', [('favorite', 'JJ'), ('player', 'NN')]), Tree('VP', [Tree('V', [('was', 'VBD')])]), Tree('VP', [Tree('V', [('playing', 'VBG')]), Tree('NP', [('this', 'DT'), ('match', 'NN')])]), ('and', 'CC'), Tree('VP', [Tree('V', [('started', 'VBD')])]), ('off', 'RP'), Tree('NP', [('strongggg', 'NN')]), (',', ','), ('it', 'PRP'), Tree('VP', [Tree('V', [('went', 'VBD')]), Tree('NP', [('downhill', 'NN')])]), Tree('P', [('after', 'IN')]), ('Messi', 'NNP'), (\"'s\", 'POS'), Tree('NP', [('injyry', 'NN'), ('midgame', 'NN')]), ('.', '.')]), Tree('S', [('My', 'PRP$'), Tree('NP', [('uncle', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('brother', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('neighbor', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('cat', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('veterinarian', 'JJ')]), ('David', 'NNP'), Tree('VP', [Tree('V', [('reads', 'VBZ')]), Tree('NP', [('the', 'DT'), ('communist', 'NN'), ('manifesto', 'NN')])]), Tree('P', [('in', 'IN')]), ('his', 'PRP$'), Tree('NP', [('spare', 'JJ'), ('time', 'NN')]), ('.', '.')]), Tree('S', [('He', 'PRP'), Tree('VP', [Tree('V', [('said', 'VBD')]), Tree('PP', [Tree('P', [('that', 'IN')]), Tree('NP', [('The', 'DT')])])]), ('Great', 'NNP'), ('Gatsby', 'NNP'), Tree('VP', [Tree('V', [('is', 'VBZ')]), Tree('NP', [('the', 'DT')])]), ('best', 'JJS'), Tree('NP', [('novell', 'NN')]), ('ever', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('VP', [Tree('V', [('was', 'VBD')])]), ('about', 'RB'), ('to', 'TO'), Tree('VP', [Tree('V', [('throw', 'VB')])]), ('hands', 'NNS'), ('.', '.')]), Tree('S', [('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), Tree('VP', [Tree('V', [('look', 'VB')])]), ('away', 'RB'), Tree('PP', [Tree('P', [('from', 'IN')]), Tree('NP', [('this', 'DT'), ('train', 'NN'), ('wrck', 'NN')])]), Tree('PP', [Tree('P', [('of', 'IN')]), Tree('NP', [('a', 'DT'), ('movie', 'NN')])]), (',', ','), Tree('P', [('on', 'IN')]), ('February', 'NNP'), ('14th', 'CD'), Tree('PP', [Tree('P', [('of', 'IN')]), Tree('NP', [('all', 'DT')])]), ('days', 'NNS'), ('.', '.')]), Tree('S', [Tree('NP', [('The', 'DT'), ('film', 'NN')]), ('Everything', 'NNP'), ('Everywhere', 'NNP'), ('All', 'NNP'), Tree('P', [('At', 'IN')]), ('Once', 'NNP'), Tree('VP', [Tree('V', [('follows', 'VBZ')])]), ('Evelyn', 'NNP'), ('Wang', 'NNP'), (',', ','), Tree('NP', [('a', 'DT'), ('woman', 'NN')]), Tree('VP', [Tree('V', [('drowning', 'VBG')]), Tree('PP', [Tree('P', [('under', 'IN')]), Tree('NP', [('the', 'DT'), ('stress', 'NN')])])]), Tree('P', [('of', 'IN')]), ('her', 'PRP$'), Tree('NP', [('family', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('failing', 'JJ'), ('laundromat', 'NN')]), ('.', '.')]), Tree('S', [('I', 'PRP'), ('just', 'RB'), Tree('VP', [Tree('V', [('finished', 'VBD')])]), Tree('VP', [Tree('V', [('reading', 'VBG')]), Tree('NP', [('pride', 'NN')])]), ('and', 'CC'), Tree('NP', [('prejudice', 'NN')]), ('which', 'WDT'), Tree('VP', [Tree('V', [('had', 'VBD')])]), ('me', 'PRP'), ('HOOOKED', 'NNP'), Tree('PP', [Tree('P', [('from', 'IN')]), Tree('NP', [('the', 'DT'), ('beginning', 'NN')])]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_output_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the RegexpParser so that it also detects Named Entity Phrases (NEP), e.g., that it detects *Galaxy S III* and *Ice Cream Sandwich*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser_v2 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "NEP: {}             # ???''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  would/MD\n",
      "  n't/RB\n",
      "  (VP (V be/VB))\n",
      "  (VP (V caught/VBN) (NP dead/JJ))\n",
      "  (VP (V watching/VBG) (NP the/DT))\n",
      "  NFL/NNP\n",
      "  (P if/IN)\n",
      "  it/PRP\n",
      "  (VP (V were/VBD))\n",
      "  n't/RB\n",
      "  (P for/IN)\n",
      "  Taylor/NNP\n",
      "  Swift/NNP\n",
      "  ./.)\n",
      "(S\n",
      "  Chris/NNP\n",
      "  O'Donnell/NNP\n",
      "  (VP (V stated/VBD))\n",
      "  (P that/IN)\n",
      "  (P while/IN)\n",
      "  (VP (V filming/VBG) (PP (P for/IN) (NP this/DT movie/NN)))\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP (V felt/VBD))\n",
      "  (P like/IN)\n",
      "  he/PRP\n",
      "  (VP (V was/VBD) (PP (P in/IN) (NP a/DT)))\n",
      "  Toys/NNP\n",
      "  ``/``\n",
      "  R/NNP\n",
      "  ''/''\n",
      "  Us/NNP\n",
      "  (NP commercial/JJ)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT whole/JJ game/NN)\n",
      "  (VP (V was/VBD) (NP a/DT rollercoaster/JJ ride/NN))\n",
      "  ,/,\n",
      "  but/CC\n",
      "  Los/NNP\n",
      "  Angeles/NNP\n",
      "  Lakers/NNP\n",
      "  ultimately/RB\n",
      "  (VP (V persevered/VBD))\n",
      "  and/CC\n",
      "  (VP (V won/VBD))\n",
      "  !/.)\n",
      "(S\n",
      "  Zendaya/NNP\n",
      "  (VP (V slayed/VBD))\n",
      "  (P in/IN)\n",
      "  Dune/NNP\n",
      "  2/CD\n",
      "  ,/,\n",
      "  (P as/IN)\n",
      "  she/PRP\n",
      "  (VP (V does/VBZ) (PP (P in/IN) (NP all/DT)))\n",
      "  her/PRP$\n",
      "  movies/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (P While/IN)\n",
      "  my/PRP$\n",
      "  (NP favorite/JJ player/NN)\n",
      "  (VP (V was/VBD))\n",
      "  (VP (V playing/VBG) (NP this/DT match/NN))\n",
      "  and/CC\n",
      "  (VP (V started/VBD))\n",
      "  off/RP\n",
      "  (NP strongggg/NN)\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  (VP (V went/VBD) (NP downhill/NN))\n",
      "  (P after/IN)\n",
      "  Messi/NNP\n",
      "  's/POS\n",
      "  (NP injyry/NN midgame/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  My/PRP$\n",
      "  (NP uncle/NN)\n",
      "  's/POS\n",
      "  (NP brother/NN)\n",
      "  's/POS\n",
      "  (NP neighbor/NN)\n",
      "  's/POS\n",
      "  (NP cat/NN)\n",
      "  's/POS\n",
      "  (NP veterinarian/JJ)\n",
      "  David/NNP\n",
      "  (VP (V reads/VBZ) (NP the/DT communist/NN manifesto/NN))\n",
      "  (P in/IN)\n",
      "  his/PRP$\n",
      "  (NP spare/JJ time/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  (VP (V said/VBD) (PP (P that/IN) (NP The/DT)))\n",
      "  Great/NNP\n",
      "  Gatsby/NNP\n",
      "  (VP (V is/VBZ) (NP the/DT))\n",
      "  best/JJS\n",
      "  (NP novell/NN)\n",
      "  ever/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (VP (V was/VBD))\n",
      "  about/RB\n",
      "  to/TO\n",
      "  (VP (V throw/VB))\n",
      "  hands/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  could/MD\n",
      "  not/RB\n",
      "  (VP (V look/VB))\n",
      "  away/RB\n",
      "  (PP (P from/IN) (NP this/DT train/NN wrck/NN))\n",
      "  (PP (P of/IN) (NP a/DT movie/NN))\n",
      "  ,/,\n",
      "  (P on/IN)\n",
      "  February/NNP\n",
      "  14th/CD\n",
      "  (PP (P of/IN) (NP all/DT))\n",
      "  days/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT film/NN)\n",
      "  Everything/NNP\n",
      "  Everywhere/NNP\n",
      "  All/NNP\n",
      "  (P At/IN)\n",
      "  Once/NNP\n",
      "  (VP (V follows/VBZ))\n",
      "  Evelyn/NNP\n",
      "  Wang/NNP\n",
      "  ,/,\n",
      "  (NP a/DT woman/NN)\n",
      "  (VP (V drowning/VBG) (PP (P under/IN) (NP the/DT stress/NN)))\n",
      "  (P of/IN)\n",
      "  her/PRP$\n",
      "  (NP family/NN)\n",
      "  's/POS\n",
      "  (NP failing/JJ laundromat/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  I/PRP\n",
      "  just/RB\n",
      "  (VP (V finished/VBD))\n",
      "  (VP (V reading/VBG) (NP pride/NN))\n",
      "  and/CC\n",
      "  (NP prejudice/NN)\n",
      "  which/WDT\n",
      "  (VP (V had/VBD))\n",
      "  me/PRP\n",
      "  HOOOKED/NNP\n",
      "  (PP (P from/IN) (NP the/DT beginning/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "constituency_v2_output_per_sentence = []\n",
    "for tokens in tokens_per_sentence:\n",
    "    constituency_output = constituent_parser_v2.parse(nltk.pos_tag(tokens))\n",
    "    constituency_v2_output_per_sentence.append(constituency_output)\n",
    "    print(constituency_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('I', 'PRP'), ('would', 'MD'), (\"n't\", 'RB'), Tree('VP', [Tree('V', [('be', 'VB')])]), Tree('VP', [Tree('V', [('caught', 'VBN')]), Tree('NP', [('dead', 'JJ')])]), Tree('VP', [Tree('V', [('watching', 'VBG')]), Tree('NP', [('the', 'DT')])]), ('NFL', 'NNP'), Tree('P', [('if', 'IN')]), ('it', 'PRP'), Tree('VP', [Tree('V', [('were', 'VBD')])]), (\"n't\", 'RB'), Tree('P', [('for', 'IN')]), ('Taylor', 'NNP'), ('Swift', 'NNP'), ('.', '.')]), Tree('S', [('Chris', 'NNP'), (\"O'Donnell\", 'NNP'), Tree('VP', [Tree('V', [('stated', 'VBD')])]), Tree('P', [('that', 'IN')]), Tree('P', [('while', 'IN')]), Tree('VP', [Tree('V', [('filming', 'VBG')]), Tree('PP', [Tree('P', [('for', 'IN')]), Tree('NP', [('this', 'DT'), ('movie', 'NN')])])]), (',', ','), ('he', 'PRP'), Tree('VP', [Tree('V', [('felt', 'VBD')])]), Tree('P', [('like', 'IN')]), ('he', 'PRP'), Tree('VP', [Tree('V', [('was', 'VBD')]), Tree('PP', [Tree('P', [('in', 'IN')]), Tree('NP', [('a', 'DT')])])]), ('Toys', 'NNP'), ('``', '``'), ('R', 'NNP'), (\"''\", \"''\"), ('Us', 'NNP'), Tree('NP', [('commercial', 'JJ')]), ('.', '.')]), Tree('S', [Tree('NP', [('The', 'DT'), ('whole', 'JJ'), ('game', 'NN')]), Tree('VP', [Tree('V', [('was', 'VBD')]), Tree('NP', [('a', 'DT'), ('rollercoaster', 'JJ'), ('ride', 'NN')])]), (',', ','), ('but', 'CC'), ('Los', 'NNP'), ('Angeles', 'NNP'), ('Lakers', 'NNP'), ('ultimately', 'RB'), Tree('VP', [Tree('V', [('persevered', 'VBD')])]), ('and', 'CC'), Tree('VP', [Tree('V', [('won', 'VBD')])]), ('!', '.')]), Tree('S', [('Zendaya', 'NNP'), Tree('VP', [Tree('V', [('slayed', 'VBD')])]), Tree('P', [('in', 'IN')]), ('Dune', 'NNP'), ('2', 'CD'), (',', ','), Tree('P', [('as', 'IN')]), ('she', 'PRP'), Tree('VP', [Tree('V', [('does', 'VBZ')]), Tree('PP', [Tree('P', [('in', 'IN')]), Tree('NP', [('all', 'DT')])])]), ('her', 'PRP$'), ('movies', 'NNS'), ('.', '.')]), Tree('S', [Tree('P', [('While', 'IN')]), ('my', 'PRP$'), Tree('NP', [('favorite', 'JJ'), ('player', 'NN')]), Tree('VP', [Tree('V', [('was', 'VBD')])]), Tree('VP', [Tree('V', [('playing', 'VBG')]), Tree('NP', [('this', 'DT'), ('match', 'NN')])]), ('and', 'CC'), Tree('VP', [Tree('V', [('started', 'VBD')])]), ('off', 'RP'), Tree('NP', [('strongggg', 'NN')]), (',', ','), ('it', 'PRP'), Tree('VP', [Tree('V', [('went', 'VBD')]), Tree('NP', [('downhill', 'NN')])]), Tree('P', [('after', 'IN')]), ('Messi', 'NNP'), (\"'s\", 'POS'), Tree('NP', [('injyry', 'NN'), ('midgame', 'NN')]), ('.', '.')]), Tree('S', [('My', 'PRP$'), Tree('NP', [('uncle', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('brother', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('neighbor', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('cat', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('veterinarian', 'JJ')]), ('David', 'NNP'), Tree('VP', [Tree('V', [('reads', 'VBZ')]), Tree('NP', [('the', 'DT'), ('communist', 'NN'), ('manifesto', 'NN')])]), Tree('P', [('in', 'IN')]), ('his', 'PRP$'), Tree('NP', [('spare', 'JJ'), ('time', 'NN')]), ('.', '.')]), Tree('S', [('He', 'PRP'), Tree('VP', [Tree('V', [('said', 'VBD')]), Tree('PP', [Tree('P', [('that', 'IN')]), Tree('NP', [('The', 'DT')])])]), ('Great', 'NNP'), ('Gatsby', 'NNP'), Tree('VP', [Tree('V', [('is', 'VBZ')]), Tree('NP', [('the', 'DT')])]), ('best', 'JJS'), Tree('NP', [('novell', 'NN')]), ('ever', 'RB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('VP', [Tree('V', [('was', 'VBD')])]), ('about', 'RB'), ('to', 'TO'), Tree('VP', [Tree('V', [('throw', 'VB')])]), ('hands', 'NNS'), ('.', '.')]), Tree('S', [('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), Tree('VP', [Tree('V', [('look', 'VB')])]), ('away', 'RB'), Tree('PP', [Tree('P', [('from', 'IN')]), Tree('NP', [('this', 'DT'), ('train', 'NN'), ('wrck', 'NN')])]), Tree('PP', [Tree('P', [('of', 'IN')]), Tree('NP', [('a', 'DT'), ('movie', 'NN')])]), (',', ','), Tree('P', [('on', 'IN')]), ('February', 'NNP'), ('14th', 'CD'), Tree('PP', [Tree('P', [('of', 'IN')]), Tree('NP', [('all', 'DT')])]), ('days', 'NNS'), ('.', '.')]), Tree('S', [Tree('NP', [('The', 'DT'), ('film', 'NN')]), ('Everything', 'NNP'), ('Everywhere', 'NNP'), ('All', 'NNP'), Tree('P', [('At', 'IN')]), ('Once', 'NNP'), Tree('VP', [Tree('V', [('follows', 'VBZ')])]), ('Evelyn', 'NNP'), ('Wang', 'NNP'), (',', ','), Tree('NP', [('a', 'DT'), ('woman', 'NN')]), Tree('VP', [Tree('V', [('drowning', 'VBG')]), Tree('PP', [Tree('P', [('under', 'IN')]), Tree('NP', [('the', 'DT'), ('stress', 'NN')])])]), Tree('P', [('of', 'IN')]), ('her', 'PRP$'), Tree('NP', [('family', 'NN')]), (\"'s\", 'POS'), Tree('NP', [('failing', 'JJ'), ('laundromat', 'NN')]), ('.', '.')]), Tree('S', [('I', 'PRP'), ('just', 'RB'), Tree('VP', [Tree('V', [('finished', 'VBD')])]), Tree('VP', [Tree('V', [('reading', 'VBG')]), Tree('NP', [('pride', 'NN')])]), ('and', 'CC'), Tree('NP', [('prejudice', 'NN')]), ('which', 'WDT'), Tree('VP', [Tree('V', [('had', 'VBD')])]), ('me', 'PRP'), ('HOOOKED', 'NNP'), Tree('PP', [Tree('P', [('from', 'IN')]), Tree('NP', [('the', 'DT'), ('beginning', 'NN')])]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_v2_output_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 1] Exercise 2: spaCy\n",
    "Use Spacy to process the same text as you analyzed with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/owhy/Library/Python/3.12/lib/python/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using SpaCy model: core_web_sm\n"
     ]
    }
   ],
   "source": [
    "model_name = nlp.meta[\"name\"]\n",
    "print(\"Currently using SpaCy model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 : I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift.\n",
      "\n",
      "Sentence 2 : Chris O'Donnell stated that while filming for this movie, he felt like he was in a Toys ''R'' Us commercial.\n",
      "\n",
      "Sentence 3 : The whole game was a rollercoaster ride, but Los Angeles Lakers ultimately persevered and won!\n",
      "\n",
      "Sentence 4 : Zendaya slayed in Dune 2, as she does in all her movies.\n",
      "\n",
      "Sentence 5 : While my favorite player was playing this match and started off strongggg, it went downhill after Messi's injyry midgame.\n",
      "\n",
      "Sentence 6 : My uncle's brother's neighbor's cat's veterinarian David reads the communist manifesto in his spare time.\n",
      "\n",
      "Sentence 7 : He said that The Great Gatsby is the best novell ever, and I was about to throw hands.\n",
      "\n",
      "Sentence 8 : I could not look away from this train wrck of a movie, on February 14th of all days.\n",
      "\n",
      "Sentence 9 : The film Everything Everywhere All At Once follows Evelyn Wang, a woman drowning under the stress of her family's failing laundromat.\n",
      "\n",
      "Sentence 10 : I just finished reading pride and prejudice which had me HOOOKED from the beginning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text) \n",
    "\n",
    "# insert code here\n",
    "sents = list(doc.sents)\n",
    "for i, sentence in enumerate(sents):\n",
    "    print('Sentence', i+1, ':', sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging:\n",
      "[('I', 'PRON'), ('would', 'AUX'), (\"n't\", 'PART'), ('be', 'AUX'), ('caught', 'VERB'), ('dead', 'ADJ'), ('watching', 'VERB'), ('the', 'DET'), ('NFL', 'PROPN'), ('if', 'SCONJ'), ('it', 'PRON'), ('were', 'AUX'), (\"n't\", 'PART'), ('for', 'ADP'), ('Taylor', 'PROPN'), ('Swift', 'PROPN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Chris', 'PROPN'), (\"O'Donnell\", 'PROPN'), ('stated', 'VERB'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('filming', 'VERB'), ('for', 'ADP'), ('this', 'DET'), ('movie', 'NOUN'), (',', 'PUNCT'), ('he', 'PRON'), ('felt', 'VERB'), ('like', 'SCONJ'), ('he', 'PRON'), ('was', 'AUX'), ('in', 'ADP'), ('a', 'DET'), ('Toys', 'PROPN'), (\"''\", 'PUNCT'), ('R', 'PROPN'), (\"''\", 'PUNCT'), ('Us', 'PROPN'), ('commercial', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('The', 'DET'), ('whole', 'ADJ'), ('game', 'NOUN'), ('was', 'AUX'), ('a', 'DET'), ('rollercoaster', 'ADJ'), ('ride', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('Los', 'PROPN'), ('Angeles', 'PROPN'), ('Lakers', 'PROPN'), ('ultimately', 'ADV'), ('persevered', 'VERB'), ('and', 'CCONJ'), ('won', 'VERB'), ('!', 'PUNCT'), ('\\n', 'SPACE'), ('Zendaya', 'PROPN'), ('slayed', 'VERB'), ('in', 'ADP'), ('Dune', 'PROPN'), ('2', 'NUM'), (',', 'PUNCT'), ('as', 'SCONJ'), ('she', 'PRON'), ('does', 'VERB'), ('in', 'ADP'), ('all', 'DET'), ('her', 'PRON'), ('movies', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('While', 'SCONJ'), ('my', 'PRON'), ('favorite', 'ADJ'), ('player', 'NOUN'), ('was', 'AUX'), ('playing', 'VERB'), ('this', 'DET'), ('match', 'NOUN'), ('and', 'CCONJ'), ('started', 'VERB'), ('off', 'ADP'), ('strongggg', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('went', 'VERB'), ('downhill', 'ADV'), ('after', 'ADP'), ('Messi', 'PROPN'), (\"'s\", 'PART'), ('injyry', 'ADJ'), ('midgame', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('My', 'PRON'), ('uncle', 'NOUN'), (\"'s\", 'PART'), ('brother', 'NOUN'), (\"'s\", 'PART'), ('neighbor', 'NOUN'), (\"'s\", 'PART'), ('cat', 'NOUN'), (\"'s\", 'PART'), ('veterinarian', 'NOUN'), ('David', 'PROPN'), ('reads', 'VERB'), ('the', 'DET'), ('communist', 'ADJ'), ('manifesto', 'NOUN'), ('in', 'ADP'), ('his', 'PRON'), ('spare', 'ADJ'), ('time', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('He', 'PRON'), ('said', 'VERB'), ('that', 'SCONJ'), ('The', 'DET'), ('Great', 'PROPN'), ('Gatsby', 'PROPN'), ('is', 'AUX'), ('the', 'DET'), ('best', 'ADJ'), ('novell', 'NOUN'), ('ever', 'ADV'), (',', 'PUNCT'), ('and', 'CCONJ'), ('I', 'PRON'), ('was', 'AUX'), ('about', 'ADJ'), ('to', 'PART'), ('throw', 'VERB'), ('hands', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('I', 'PRON'), ('could', 'AUX'), ('not', 'PART'), ('look', 'VERB'), ('away', 'ADV'), ('from', 'ADP'), ('this', 'DET'), ('train', 'NOUN'), ('wrck', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('movie', 'NOUN'), (',', 'PUNCT'), ('on', 'ADP'), ('February', 'PROPN'), ('14th', 'NOUN'), ('of', 'ADP'), ('all', 'DET'), ('days', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('The', 'DET'), ('film', 'NOUN'), ('Everything', 'PRON'), ('Everywhere', 'ADV'), ('All', 'PRON'), ('At', 'ADP'), ('Once', 'ADV'), ('follows', 'VERB'), ('Evelyn', 'PROPN'), ('Wang', 'PROPN'), (',', 'PUNCT'), ('a', 'DET'), ('woman', 'NOUN'), ('drowning', 'VERB'), ('under', 'ADP'), ('the', 'DET'), ('stress', 'NOUN'), ('of', 'ADP'), ('her', 'PRON'), ('family', 'NOUN'), (\"'s\", 'PART'), ('failing', 'VERB'), ('laundromat', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('I', 'PRON'), ('just', 'ADV'), ('finished', 'VERB'), ('reading', 'VERB'), ('pride', 'NOUN'), ('and', 'CCONJ'), ('prejudice', 'NOUN'), ('which', 'PRON'), ('had', 'VERB'), ('me', 'PRON'), ('HOOOKED', 'PROPN'), ('from', 'ADP'), ('the', 'DET'), ('beginning', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE')]\n",
      "NER:\n",
      "[('NFL', 'ORG'), ('Taylor Swift', 'PERSON'), (\"Chris O'Donnell\", 'PERSON'), ('Us', 'GPE'), ('Los Angeles Lakers', 'ORG'), ('Zendaya', 'GPE'), ('Dune 2', 'LAW'), ('Messi', 'PERSON'), ('David', 'PERSON'), ('The Great Gatsby', 'WORK_OF_ART'), ('February 14th of all days', 'DATE'), ('Everything Everywhere All At Once', 'WORK_OF_ART'), ('Evelyn Wang', 'PERSON')]\n",
      "Constituency Parsing:\n",
      "[('I', 'nsubjpass'), ('would', 'aux'), (\"n't\", 'neg'), ('be', 'auxpass'), ('caught', 'ROOT'), ('dead', 'oprd'), ('watching', 'advcl'), ('the', 'det'), ('NFL', 'dobj'), ('if', 'mark'), ('it', 'nsubj'), ('were', 'advcl'), (\"n't\", 'neg'), ('for', 'prep'), ('Taylor', 'compound'), ('Swift', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('Chris', 'compound'), (\"O'Donnell\", 'nsubj'), ('stated', 'ROOT'), ('that', 'mark'), ('while', 'mark'), ('filming', 'advcl'), ('for', 'prep'), ('this', 'det'), ('movie', 'pobj'), (',', 'punct'), ('he', 'nsubj'), ('felt', 'ccomp'), ('like', 'mark'), ('he', 'nsubj'), ('was', 'advcl'), ('in', 'prep'), ('a', 'det'), ('Toys', 'compound'), (\"''\", 'punct'), ('R', 'nmod'), (\"''\", 'punct'), ('Us', 'compound'), ('commercial', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('The', 'det'), ('whole', 'amod'), ('game', 'nsubj'), ('was', 'ROOT'), ('a', 'det'), ('rollercoaster', 'amod'), ('ride', 'attr'), (',', 'punct'), ('but', 'cc'), ('Los', 'compound'), ('Angeles', 'compound'), ('Lakers', 'nsubj'), ('ultimately', 'advmod'), ('persevered', 'conj'), ('and', 'cc'), ('won', 'conj'), ('!', 'punct'), ('\\n', 'dep'), ('Zendaya', 'nsubj'), ('slayed', 'ROOT'), ('in', 'prep'), ('Dune', 'pobj'), ('2', 'nummod'), (',', 'punct'), ('as', 'mark'), ('she', 'nsubj'), ('does', 'advcl'), ('in', 'prep'), ('all', 'predet'), ('her', 'poss'), ('movies', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('While', 'mark'), ('my', 'poss'), ('favorite', 'amod'), ('player', 'nsubj'), ('was', 'aux'), ('playing', 'advcl'), ('this', 'det'), ('match', 'dobj'), ('and', 'cc'), ('started', 'conj'), ('off', 'prt'), ('strongggg', 'pobj'), (',', 'punct'), ('it', 'nsubj'), ('went', 'ROOT'), ('downhill', 'advmod'), ('after', 'prep'), ('Messi', 'poss'), (\"'s\", 'case'), ('injyry', 'amod'), ('midgame', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('My', 'poss'), ('uncle', 'poss'), (\"'s\", 'case'), ('brother', 'poss'), (\"'s\", 'case'), ('neighbor', 'poss'), (\"'s\", 'case'), ('cat', 'poss'), (\"'s\", 'case'), ('veterinarian', 'nsubj'), ('David', 'appos'), ('reads', 'ROOT'), ('the', 'det'), ('communist', 'amod'), ('manifesto', 'dobj'), ('in', 'prep'), ('his', 'poss'), ('spare', 'amod'), ('time', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('He', 'nsubj'), ('said', 'ROOT'), ('that', 'mark'), ('The', 'det'), ('Great', 'compound'), ('Gatsby', 'nsubj'), ('is', 'ccomp'), ('the', 'det'), ('best', 'amod'), ('novell', 'attr'), ('ever', 'advmod'), (',', 'punct'), ('and', 'cc'), ('I', 'nsubj'), ('was', 'conj'), ('about', 'acomp'), ('to', 'aux'), ('throw', 'xcomp'), ('hands', 'dobj'), ('.', 'punct'), ('\\n', 'dep'), ('I', 'nsubj'), ('could', 'aux'), ('not', 'neg'), ('look', 'ROOT'), ('away', 'advmod'), ('from', 'prep'), ('this', 'det'), ('train', 'compound'), ('wrck', 'pobj'), ('of', 'prep'), ('a', 'det'), ('movie', 'pobj'), (',', 'punct'), ('on', 'prep'), ('February', 'compound'), ('14th', 'pobj'), ('of', 'prep'), ('all', 'det'), ('days', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('The', 'det'), ('film', 'nsubj'), ('Everything', 'appos'), ('Everywhere', 'advmod'), ('All', 'appos'), ('At', 'prep'), ('Once', 'pobj'), ('follows', 'ROOT'), ('Evelyn', 'compound'), ('Wang', 'dobj'), (',', 'punct'), ('a', 'det'), ('woman', 'appos'), ('drowning', 'acl'), ('under', 'prep'), ('the', 'det'), ('stress', 'pobj'), ('of', 'prep'), ('her', 'poss'), ('family', 'poss'), (\"'s\", 'case'), ('failing', 'amod'), ('laundromat', 'pobj'), ('.', 'punct'), ('\\n', 'dep'), ('I', 'nsubj'), ('just', 'advmod'), ('finished', 'ROOT'), ('reading', 'xcomp'), ('pride', 'dobj'), ('and', 'cc'), ('prejudice', 'conj'), ('which', 'nsubj'), ('had', 'relcl'), ('me', 'dobj'), ('HOOOKED', 'dobj'), ('from', 'prep'), ('the', 'det'), ('beginning', 'pobj'), ('.', 'punct'), ('\\n', 'dep')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS tagging:\")\n",
    "print(pos_tags)\n",
    "\n",
    "# NER\n",
    "ner_results = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"NER:\")\n",
    "print(ner_results)\n",
    "\n",
    "# constituency parsing (syntactic dependency parsing using the `dep_`)\n",
    "constituency_parsing = [(token.text, token.dep_) for token in doc]\n",
    "print(\"Constituency Parsing:\")\n",
    "print(constituency_parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/owhy/Documents/GitHub/TextMining-VU-2024/lab4/CONLL2003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader(filename, \"train.txt\", [\"words\", \"pos\", \"ignore\", \"chunk\"])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        \"word\": token,\n",
    "        \"pos\": pos,\n",
    "        \"word.lower()\": token.lower(),  # lower case variant of the token\n",
    "        \"suffix-2char\": token[-2:],  # suffix of 2 characters\n",
    "        \"capitalization\": token.isupper(),  # initial captial\n",
    "        \"capitalization_all\": token.istitle(),  # all words ini caps\n",
    "        \"digit\": token.isdigit(),\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader(filename, \"test.txt\", [\"words\", \"pos\", \"ignore\", \"chunk\"])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        \"word\": token,\n",
    "        \"pos\": pos,\n",
    "        \"word.lower()\": token.lower(),  # lower case variant of the token\n",
    "        \"suffix-2char\": token[-2:],  # suffix of 2 characters\n",
    "        \"capitalization\": token.isupper(),  # initial captial\n",
    "        \"capitalization_all\": token.istitle(),  # all words ini caps\n",
    "        \"digit\": token.isdigit(),\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Examples:\n",
      "Features: {'word': 'EU', 'pos': 'NNP', 'word.lower()': 'eu', 'suffix-2char': 'EU', 'capitalization': True, 'capitalization_all': False, 'digit': False}\n",
      "Gold Label: B-ORG\n",
      "\n",
      "Features: {'word': 'rejects', 'pos': 'VBZ', 'word.lower()': 'rejects', 'suffix-2char': 'ts', 'capitalization': False, 'capitalization_all': False, 'digit': False}\n",
      "Gold Label: O\n",
      "\n",
      "Test Data Examples:\n",
      "Features: {'word': 'SOCCER', 'pos': 'NN', 'word.lower()': 'soccer', 'suffix-2char': 'ER', 'capitalization': True, 'capitalization_all': False, 'digit': False}\n",
      "Gold Label: O\n",
      "\n",
      "Features: {'word': '-', 'pos': ':', 'word.lower()': '-', 'suffix-2char': '-', 'capitalization': False, 'capitalization_all': False, 'digit': False}\n",
      "Gold Label: O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print examples from the training data\n",
    "print(\"Training Data Examples:\")\n",
    "for i in range(2):\n",
    "    print(\"Features:\", training_features[i])\n",
    "    print(\"Gold Label:\", training_gold_labels[i])\n",
    "    print()\n",
    "\n",
    "# Print examples from the test data\n",
    "print(\"Test Data Examples:\")\n",
    "for i in range(2):  # Iterate over the entire test_features list\n",
    "    print(\"Features:\", test_features[i])\n",
    "    print(\"Gold Label:\", test_gold_labels[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in train: 203621\n",
      "Number of instances in test: 46435\n",
      "\n",
      "Frequency distribution of NERC labels in train:\n",
      "Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n",
      "\n",
      "Frequency distribution of NERC labels in test:\n",
      "Counter({'O': 38323, 'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216})\n",
      "\n",
      "Balance of NERC labels in train data:\n",
      "{'B-ORG': 0.03104296708099852, 'O': 0.832811939829389, 'B-MISC': 0.016884309575142052, 'B-PER': 0.0324131597428556, 'I-PER': 0.022237392017522752, 'B-LOC': 0.03506514553999833, 'I-ORG': 0.018190658134475325, 'I-MISC': 0.00567230295499973, 'I-LOC': 0.005682125124618777}\n",
      "\n",
      "Balance of NERC labels in test data:\n",
      "{'O': 0.8253041886508022, 'B-LOC': 0.03592118014428771, 'B-PER': 0.03482287067944438, 'I-PER': 0.0248950145364488, 'I-LOC': 0.005534618283622268, 'B-MISC': 0.015117906751372886, 'I-MISC': 0.004651663615807042, 'B-ORG': 0.03577043178636804, 'I-ORG': 0.017982125551846667}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each NERC label in the training data\n",
    "train_label_counts = Counter(training_gold_labels)\n",
    "\n",
    "# Count the occurrences of each NERC label in the test data\n",
    "test_label_counts = Counter(test_gold_labels)\n",
    "\n",
    "# Print the number of instances in train and test\n",
    "print(\"Number of instances in train:\", len(training_gold_labels))\n",
    "print(\"Number of instances in test:\", len(test_gold_labels))\n",
    "\n",
    "# Print the frequency distribution of NERC labels in train and test\n",
    "print(\"\\nFrequency distribution of NERC labels in train:\")\n",
    "print(train_label_counts)\n",
    "\n",
    "print(\"\\nFrequency distribution of NERC labels in test:\")\n",
    "print(test_label_counts)\n",
    "\n",
    "# Discuss the balance and differences between train and test data\n",
    "train_total_instances = len(training_gold_labels)\n",
    "test_total_instances = len(test_gold_labels)\n",
    "\n",
    "train_balance = {\n",
    "    label: count / train_total_instances for label, count in train_label_counts.items()\n",
    "}\n",
    "test_balance = {\n",
    "    label: count / test_total_instances for label, count in test_label_counts.items()\n",
    "}\n",
    "\n",
    "print(\"\\nBalance of NERC labels in train data:\")\n",
    "print(train_balance)\n",
    "\n",
    "print(\"\\nBalance of NERC labels in test data:\")\n",
    "print(test_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the training data, the 'O' label predominates, indicating non-entity tokens, while the occurrences of other NERC labels (B-LOC, B-PER, B-ORG, I-PER, I-ORG, B-MISC, I-LOC, I-MISC) are relatively fewer. This distribution is mirrored in the test data, with 'O' labels being dominant and other labels occurring less frequently.\n",
    "* Both datasets exhibit similar distributions of NERC labels, with 'O' labels being most common. However, slight differences exist in the proportions of some labels. For instance, 'B-PER' labels are slightly more prevalent in the test data, while 'B-ORG' labels are slightly less common\n",
    "* Both datasets display a class imbalance, with 'O' labels representing the majority of instances. This imbalance is more pronounced in the training data. Nonetheless, both datasets encompass instances for all NERC labels, facilitating effective learning across various entity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = training_features + test_features\n",
    "\n",
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(all_features)\n",
    "\n",
    "train_instances = len(training_features)\n",
    "train_array = the_array[:train_instances]\n",
    "test_array = the_array[train_instances:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.77      0.80      0.78      1668\n",
      "      B-MISC       0.74      0.69      0.71       702\n",
      "       B-ORG       0.72      0.57      0.64      1661\n",
      "       B-PER       0.68      0.59      0.63      1617\n",
      "       I-LOC       0.62      0.55      0.58       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.55      0.49      0.51       835\n",
      "       I-PER       0.46      0.62      0.53      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.65      0.66     46435\n",
      "weighted avg       0.93      0.93      0.93     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "# lin_clf.fit( # your code here\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lin_clf.fit(train_array, training_gold_labels)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels = lin_clf.predict(test_array)\n",
    "\n",
    "# Evaluate on test data and print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_gold_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import json\n",
    "from nltk.sentiment import vader\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = json.load(open('vader.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'sentiment_label': 'negative', 'text_of_tweet': \"I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift.\", 'topic': 'sports'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in vader.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # 'en_core_web_sm'\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "def run_vader(\n",
    "    textual_unit, lemmatize=False, parts_of_speech_to_consider=None, verbose=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "\n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "\n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == \"-PRON-\":\n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(\" \".join(input_to_vader))\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print(\"INPUT SENTENCE\", sent)\n",
    "        print(\"INPUT TO VADER\", input_to_vader)\n",
    "        print(\"VADER OUTPUT\", scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         4\n",
      "     neutral       0.33      0.33      0.33         3\n",
      "    positive       0.20      0.33      0.25         3\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.18      0.22      0.19        10\n",
      "weighted avg       0.16      0.20      0.17        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"vader.json\", \"r\") as f:\n",
    "    my_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)  # run vader\n",
    "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BERT & RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same training, development, and test partitions of the the 20 newsgroups text dataset as in Lab6.4-Topic-classification-BERT.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fine-tune and examine the performance of another transformer-based pretrained language models, e.g., RoBERTa, XLNet\n",
    "\n",
    "* Compare the performance of this model to the results achieved in Lab6.4-Topic-classification-BERT.ipynb and to a conventional machine learning approach (e.g., SVM, Naive Bayes) using bag-of-words or other engineered features of your choice. \n",
    "Describe the differences in performance in terms of Precision, Recall, and F1-score evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/owhy/Library/Python/3.12/lib/python/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict \n",
    "from math import log\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from math import log \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing dataset of 20 newsgroups with 18K posts on 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "categories = [\"alt.atheism\", \"comp.graphics\", \"sci.med\", \"sci.space\"]\n",
    "\n",
    "# remove the headers, footers and quotes (to avoid overfitting)\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    categories=categories,\n",
    "    random_state=42,\n",
    ")\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    categories=categories,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(newsgroups_train.target)\n",
    "\n",
    "train = pd.DataFrame({\"text\": newsgroups_train.data, \"labels\": newsgroups_train.target})\n",
    "test = pd.DataFrame({\"text\": newsgroups_test.data, \"labels\": newsgroups_test.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only a sub-selection of the categories (4 in our case)\n",
    "categories = [\"alt.atheism\", \"comp.graphics\", \"sci.med\", \"sci.space\"]\n",
    "\n",
    "# remove the headers, footers and quotes (to avoid overfitting)\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    categories=categories,\n",
    "    random_state=42,\n",
    ")\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    categories=categories,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(newsgroups_train.target)\n",
    "\n",
    "train = pd.DataFrame({\"text\": newsgroups_train.data, \"labels\": newsgroups_train.target})\n",
    "test = pd.DataFrame({\"text\": newsgroups_test.data, \"labels\": newsgroups_test.target})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 594, 3: 593, 1: 584, 0: 480})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 396, 3: 394, 1: 389, 0: 319})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'text': newsgroups_train.data, 'labels': newsgroups_train.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2251\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHile we are on the subject of the shuttle sof...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is a program called Graphic Workshop you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My girlfriend is in pain from kidney stones. S...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think that's the correct spelling..\\n\\tI am ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  WHile we are on the subject of the shuttle sof...       3\n",
       "1  There is a program called Graphic Workshop you...       1\n",
       "2                                                          2\n",
       "3  My girlfriend is in pain from kidney stones. S...       2\n",
       "4  I think that's the correct spelling..\\n\\tI am ...       2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train))\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"text\": newsgroups_test.data, \"labels\": newsgroups_test.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nAnd guess who's here in your place.\\n\\nPleas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does anyone know if any of Currier and Ives et...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>=FLAME ON\\n=\\n=Reading through the posts about...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nBut in this case I said I hoped that BCCI wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIn the kind I have made I used a Lite sour c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  \\nAnd guess who's here in your place.\\n\\nPleas...       1\n",
       "1  Does anyone know if any of Currier and Ives et...       1\n",
       "2  =FLAME ON\\n=\\n=Reading through the posts about...       2\n",
       "3  \\nBut in this case I said I hoped that BCCI wa...       0\n",
       "4  \\nIn the kind I have made I used a Lite sour c...       2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test))\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, dev = train_test_split(\n",
    "    train, test_size=0.1, random_state=0, stratify=train[[\"labels\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "train: labels\n",
      "0         432\n",
      "1         525\n",
      "2         534\n",
      "3         534\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>I wonder how many atheists out there care to s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>We are interested in purchasing a grayscale pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>Dear Binary Newsers,\\n\\nI am looking for Quick...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "559   I wonder how many atheists out there care to s...       0\n",
       "2060  We are interested in purchasing a grayscale pr...       1\n",
       "1206  Dear Binary Newsers,\\n\\nI am looking for Quick...       1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(\"train:\", train[[\"labels\"]].value_counts(sort=False))\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "dev: labels\n",
      "0         48\n",
      "1         59\n",
      "2         60\n",
      "3         59\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>I'd dump him.  Rude is rude and it seems he en...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>A friend of mine has been diagnosed with Psori...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "1570  I'd dump him.  Rude is rude and it seems he en...       2\n",
       "1761  Hi Everyone ::\\n\\nI am  looking for  some soft...       1\n",
       "455   A friend of mine has been diagnosed with Psori...       2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dev))\n",
    "print(\"dev:\", dev[[\"labels\"]].value_counts(sort=False))\n",
    "dev.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "train_texts = train[\"text\"].tolist()\n",
    "train_labels = train[\"labels\"].tolist()\n",
    "dev_texts = dev[\"text\"].tolist()\n",
    "dev_labels = dev[\"labels\"].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings[\"input_ids\"]),\n",
    "    torch.tensor(train_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(train_labels),\n",
    ")\n",
    "dev_dataset = TensorDataset(\n",
    "    torch.tensor(dev_encodings[\"input_ids\"]),\n",
    "    torch.tensor(dev_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(dev_labels),\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 0.1 * len(train_dataset) / batch_size\n",
    "logging_steps = 100\n",
    "eval_steps = 500\n",
    "early_stopping_patience = 2\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler, batch_size=batch_size\n",
    ")\n",
    "dev_sampler = SequentialSampler(dev_dataset)\n",
    "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=len(train_dataloader) * epochs,\n",
    ")\n",
    "\n",
    "# Define early stopping variables\n",
    "best_dev_loss = float(\"inf\")\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 2/64 [00:09<04:45,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3756866455078125\n",
      "Step 2/64, Average Loss: 1.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 4/64 [00:17<04:10,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4117562770843506\n",
      "Step 4/64, Average Loss: 1.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 6/64 [00:27<04:43,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3861721754074097\n",
      "Step 6/64, Average Loss: 1.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▎        | 8/64 [00:37<04:40,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.403000831604004\n",
      "Step 8/64, Average Loss: 1.4030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 10/64 [00:45<04:00,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3720635771751404\n",
      "Step 10/64, Average Loss: 1.3721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 12/64 [00:53<03:43,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.377163290977478\n",
      "Step 12/64, Average Loss: 1.3772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 14/64 [01:00<03:16,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3783198595046997\n",
      "Step 14/64, Average Loss: 1.3783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 16/64 [01:08<03:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2825502753257751\n",
      "Step 16/64, Average Loss: 1.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 18/64 [01:16<03:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.191789150238037\n",
      "Step 18/64, Average Loss: 1.1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███▏      | 20/64 [01:24<02:54,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.010626345872879\n",
      "Step 20/64, Average Loss: 1.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 22/64 [01:32<02:50,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836635023355484\n",
      "Step 22/64, Average Loss: 0.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 24/64 [01:43<03:11,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7703510522842407\n",
      "Step 24/64, Average Loss: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 26/64 [01:53<03:05,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6823537945747375\n",
      "Step 26/64, Average Loss: 0.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 28/64 [02:02<02:54,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644910603761673\n",
      "Step 28/64, Average Loss: 0.6449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 30/64 [02:09<02:18,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5166327953338623\n",
      "Step 30/64, Average Loss: 0.5166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 32/64 [02:16<01:58,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46008333563804626\n",
      "Step 32/64, Average Loss: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 34/64 [02:23<01:47,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949372231960297\n",
      "Step 34/64, Average Loss: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 36/64 [02:31<01:48,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5730343908071518\n",
      "Step 36/64, Average Loss: 0.5730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 38/64 [02:38<01:37,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5904063284397125\n",
      "Step 38/64, Average Loss: 0.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▎   | 40/64 [02:45<01:25,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7363036274909973\n",
      "Step 40/64, Average Loss: 0.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 42/64 [02:52<01:17,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5400028824806213\n",
      "Step 42/64, Average Loss: 0.5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 44/64 [02:59<01:11,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3889882415533066\n",
      "Step 44/64, Average Loss: 0.3890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 46/64 [03:09<01:16,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4161134660243988\n",
      "Step 46/64, Average Loss: 0.4161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 48/64 [03:18<01:07,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4130480885505676\n",
      "Step 48/64, Average Loss: 0.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 50/64 [03:25<00:54,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24456873536109924\n",
      "Step 50/64, Average Loss: 0.2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████▏ | 52/64 [03:33<00:46,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4144319146871567\n",
      "Step 52/64, Average Loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 54/64 [03:41<00:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5335825383663177\n",
      "Step 54/64, Average Loss: 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 56/64 [03:50<00:33,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43918849527835846\n",
      "Step 56/64, Average Loss: 0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 58/64 [03:58<00:23,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37058717012405396\n",
      "Step 58/64, Average Loss: 0.3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 60/64 [04:06<00:16,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17885372787714005\n",
      "Step 60/64, Average Loss: 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 62/64 [04:14<00:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48040080070495605\n",
      "Step 62/64, Average Loss: 0.4804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 64/64 [04:19<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21774737164378166\n",
      "Step 64/64, Average Loss: 0.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define logging steps\n",
    "logging_steps = 2\n",
    "\n",
    "# Lists to store training loss history\n",
    "train_loss_history = []\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\"\n",
    "    )\n",
    "\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (step + 1) % logging_steps == 0:\n",
    "            avg_loss = total_loss / logging_steps\n",
    "            print(avg_loss)\n",
    "            print(\n",
    "                f\"Step {step + 1}/{len(train_dataloader)}, Average Loss: {avg_loss:.4f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "            # Store training loss in history\n",
    "            train_loss_history.append(avg_loss)\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3756866455078125, 1.4117562770843506, 1.3861721754074097, 1.403000831604004, 1.3720635771751404, 1.377163290977478, 1.3783198595046997, 1.2825502753257751, 1.191789150238037, 1.010626345872879, 0.836635023355484, 0.7703510522842407, 0.6823537945747375, 0.644910603761673, 0.5166327953338623, 0.46008333563804626, 0.7949372231960297, 0.5730343908071518, 0.5904063284397125, 0.7363036274909973, 0.5400028824806213, 0.3889882415533066, 0.4161134660243988, 0.4130480885505676, 0.24456873536109924, 0.4144319146871567, 0.5335825383663177, 0.43918849527835846, 0.37058717012405396, 0.17885372787714005, 0.48040080070495605, 0.21774737164378166]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss history\n",
    "plt.plot(train_loss_history)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Topics:\n",
      "Sentence: I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift. \t Predicted Topic: sports\n",
      "Sentence: Chris O'Donnell stated that while filming for this movie, he felt like he was in a Toys ''R'' Us commercial. \t Predicted Topic: None\n",
      "Sentence: The whole game was a rollercoaster ride, but Los Angeles Lakers ultimately persevered and won! \t Predicted Topic: None\n",
      "Sentence: Zendaya slayed in Dune 2, as she does in all her movies. \t Predicted Topic: None\n",
      "Sentence: While my favorite player was playing this match and started off strongggg, it went downhill after Messi's injyry midgame. \t Predicted Topic: sports\n",
      "Sentence: My uncle's brother's neighbor's cat's veterinarian David reads the communist manifesto in his spare time. \t Predicted Topic: sports\n",
      "Sentence: He said that The Great Gatsby is the best novell ever, and I was about to throw hands. \t Predicted Topic: None\n",
      "Sentence: I could not look away from this train wrck of a movie, on February 14th of all days. \t Predicted Topic: None\n",
      "Sentence: The film Everything Everywhere All At Once follows Evelyn Wang, a woman drowning under the stress of her family's failing laundromat. \t Predicted Topic: book\n",
      "Sentence: I just finished reading pride and prejudice which had me HOOOKED from the beginning. \t Predicted Topic: sports\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "\n",
    "# Load the TSV file\n",
    "test_df = pd.read_csv(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/sentiment-topic-test.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "# Extract text and topic columns\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "\n",
    "# Rest of your evaluation code remains unchanged\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_encodings[\"input_ids\"]),\n",
    "    torch.tensor(test_encodings[\"attention_mask\"]),\n",
    ")\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).tolist()\n",
    "        print(preds)\n",
    "\n",
    "    test_preds.extend(preds)\n",
    "\n",
    "# Map topic labels to their corresponding names\n",
    "topic_mapping = {0: \"sports\", 1: \"movie\", 2: \"book\"}\n",
    "\n",
    "# Print sentences and predicted topic labels\n",
    "print(\"Predicted Topics:\")\n",
    "for sentence, prediction in zip(test_texts, test_preds):\n",
    "    predicted_topic = topic_mapping.get(prediction)\n",
    "    print(f\"Sentence: {sentence} \\t Predicted Topic: {predicted_topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sports', 1: 'movie', 2: 'book', 3: None}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 3, 3, 0, 0, 3, 3, 2, 0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set labels: 10\n",
      "Length of predictions: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_array_api.py:290: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_preds))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate classification report\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msports\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbook\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print the classification report\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2604\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2469\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2470\u001b[0m     {\n\u001b[1;32m   2471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2495\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2496\u001b[0m ):\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \n\u001b[1;32m   2499\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2604\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 86\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:389\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    387\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 389\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code for loading the test dataset and making predictions\n",
    "\n",
    "# Convert topic labels to numerical values for classification_report\n",
    "topic_mapping = {\"sports\": 0, \"movie\": 1, \"book\": 2}\n",
    "test_df[\"topic\"] = test_df[\"topic\"].map(topic_mapping)\n",
    "\n",
    "print(\"Length of test set labels:\", len(test_df[\"topic\"]))\n",
    "print(\"Length of predictions:\", len(test_preds))\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_df[\"topic\"], test_preds, target_names=[\"sports\", \"movie\", \"book\", \"none\"], output_dict=True)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Test set evaluation:\")\n",
    "print(\"\\t\".join([\"precision\", \"recall\", \"f1-score\", \"support\"]))\n",
    "for label, scores in report.items():\n",
    "    if label in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        continue\n",
    "    print(label, end=\"\\t\")\n",
    "    for metric in [\"precision\", \"recall\", \"f1-score\", \"support\"]:\n",
    "        print(f\"{scores[metric]:.2f}\", end=\"\\t\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "train_texts = train[\"text\"].tolist()\n",
    "train_labels = train[\"labels\"].tolist()\n",
    "dev_texts = dev[\"text\"].tolist()\n",
    "dev_labels = dev[\"labels\"].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings[\"input_ids\"]),\n",
    "    torch.tensor(train_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(train_labels),\n",
    ")\n",
    "dev_dataset = TensorDataset(\n",
    "    torch.tensor(dev_encodings[\"input_ids\"]),\n",
    "    torch.tensor(dev_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(dev_labels),\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 0.1 * len(train_dataset) / batch_size\n",
    "logging_steps = 100\n",
    "eval_steps = 500\n",
    "early_stopping_patience = 2\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler, batch_size=batch_size\n",
    ")\n",
    "dev_sampler = SequentialSampler(dev_dataset)\n",
    "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=len(train_dataloader) * epochs,\n",
    ")\n",
    "\n",
    "# Define early stopping variables\n",
    "best_dev_loss = float(\"inf\")\n",
    "early_stopping_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2028,  2518,  ...,  4136,  2017,   102],\n",
       "         [  101,  2009,  1005,  ...,     0,     0,     0],\n",
       "         [  101,  1011,  1028,  ...,  2094,  2114,   102],\n",
       "         ...,\n",
       "         [  101, 21213,  2937,  ...,  4367,  1010,   102],\n",
       "         [  101,  1045,  2342,  ...,  1012,  1996,   102],\n",
       "         [  101,  7632,  3148,  ...,  1996,  2739,   102]]),\n",
       " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " tensor([0, 3, 0, 1, 0, 3, 1, 2, 1, 3, 2, 2, 2, 3, 2, 3, 1, 1, 2, 1, 1, 1, 1, 2,\n",
       "         0, 1, 3, 0, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2941,  1010,  ...,  2056,  2008,   102],\n",
      "        [  101,  7175,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  2026,  6513,  ...,  1998,  2018,   102],\n",
      "        ...,\n",
      "        [  101,  3293,  2686,  ...,  1005,  1049,   102],\n",
      "        [  101,  2391,  2115,  ...,  1028,  1015,   102],\n",
      "        [  101,  1031, 17159,  ...,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/64 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 3 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1574\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1573\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1574\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1576\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 3 is out of bounds."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define logging steps\n",
    "logging_steps = 2\n",
    "\n",
    "# Lists to store training loss history\n",
    "train_loss_history = []\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\"\n",
    "    )\n",
    "\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (step + 1) % logging_steps == 0:\n",
    "            avg_loss = total_loss / logging_steps\n",
    "            print(avg_loss)\n",
    "            print(\n",
    "                f\"Step {step + 1}/{len(train_dataloader)}, Average Loss: {avg_loss:.4f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "            # Store training loss in history\n",
    "            train_loss_history.append(avg_loss)\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Topics:\n",
      "Sentence: I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift. \t Predicted Topic: sports \t Logits: [0.3628716766834259, 0.3254174590110779, 0.030501870438456535]\n",
      "Sentence: Chris O'Donnell stated that while filming for this movie, he felt like he was in a Toys ''R'' Us commercial. \t Predicted Topic: movie \t Logits: [0.2435493916273117, 0.3817989230155945, 0.05770273134112358]\n",
      "Sentence: The whole game was a rollercoaster ride, but Los Angeles Lakers ultimately persevered and won! \t Predicted Topic: sports \t Logits: [0.6979091763496399, 0.5067521929740906, 0.24804063141345978]\n",
      "Sentence: Zendaya slayed in Dune 2, as she does in all her movies. \t Predicted Topic: movie \t Logits: [0.32085931301116943, 0.3508490324020386, 0.11814739555120468]\n",
      "Sentence: While my favorite player was playing this match and started off strongggg, it went downhill after Messi's injyry midgame. \t Predicted Topic: sports \t Logits: [0.22296851873397827, 0.2097213715314865, -0.06286153942346573]\n",
      "Sentence: My uncle's brother's neighbor's cat's veterinarian David reads the communist manifesto in his spare time. \t Predicted Topic: sports \t Logits: [0.5523343682289124, 0.40848591923713684, 0.08921118825674057]\n",
      "Sentence: He said that The Great Gatsby is the best novell ever, and I was about to throw hands. \t Predicted Topic: sports \t Logits: [0.5332633852958679, 0.41934511065483093, 0.08932463824748993]\n",
      "Sentence: I could not look away from this train wrck of a movie, on February 14th of all days. \t Predicted Topic: sports \t Logits: [0.5036666393280029, 0.3347598612308502, 0.08448468893766403]\n",
      "Sentence: The film Everything Everywhere All At Once follows Evelyn Wang, a woman drowning under the stress of her family's failing laundromat. \t Predicted Topic: movie \t Logits: [0.23704174160957336, 0.4167352318763733, 0.10337252914905548]\n",
      "Sentence: I just finished reading pride and prejudice which had me HOOOKED from the beginning. \t Predicted Topic: movie \t Logits: [0.0488525927066803, 0.1496303379535675, -0.05384625121951103]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set and print logits and predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_logits = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "    test_preds.extend(preds)\n",
    "    test_logits.extend(logits.tolist())\n",
    "\n",
    "# Map topic labels to their corresponding names\n",
    "topic_mapping = {0: \"sports\", 1: \"movie\", 2: \"book\"}\n",
    "\n",
    "# Print sentences, predicted topic labels, and logits\n",
    "print(\"Predicted Topics:\")\n",
    "for sentence, prediction, logits in zip(test_texts, test_preds, test_logits):\n",
    "    predicted_topic = topic_mapping.get(prediction)\n",
    "    print(f\"Sentence: {sentence} \\t Predicted Topic: {predicted_topic} \\t Logits: {logits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sports\n",
       "1     movie\n",
       "2    sports\n",
       "3     movie\n",
       "4    sports\n",
       "5      book\n",
       "6      book\n",
       "7     movie\n",
       "8     movie\n",
       "9      book\n",
       "Name: topic, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"text\": test_df.text, \"labels\": test_df.topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wouldn't be caught dead watching the NFL if ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris O'Donnell stated that while filming for ...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The whole game was a rollercoaster ride, but L...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zendaya slayed in Dune 2, as she does in all h...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While my favorite player was playing this matc...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My uncle's brother's neighbor's cat's veterina...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>He said that The Great Gatsby is the best nove...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I could not look away from this train wrck of ...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The film Everything Everywhere All At Once fol...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I just finished reading pride and prejudice wh...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  I wouldn't be caught dead watching the NFL if ...  sports\n",
       "1  Chris O'Donnell stated that while filming for ...   movie\n",
       "2  The whole game was a rollercoaster ride, but L...  sports\n",
       "3  Zendaya slayed in Dune 2, as she does in all h...   movie\n",
       "4  While my favorite player was playing this matc...  sports\n",
       "5  My uncle's brother's neighbor's cat's veterina...    book\n",
       "6  He said that The Great Gatsby is the best nove...    book\n",
       "7  I could not look away from this train wrck of ...   movie\n",
       "8  The film Everything Everywhere All At Once fol...   movie\n",
       "9  I just finished reading pride and prejudice wh...    book"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I wouldn't be caught dead watching the NFL if it weren't for Taylor Swift.\",\n",
       " \"Chris O'Donnell stated that while filming for this movie, he felt like he was in a Toys ''R'' Us commercial.\",\n",
       " 'The whole game was a rollercoaster ride, but Los Angeles Lakers ultimately persevered and won!',\n",
       " 'Zendaya slayed in Dune 2, as she does in all her movies.',\n",
       " \"While my favorite player was playing this match and started off strongggg, it went downhill after Messi's injyry midgame.\",\n",
       " \"My uncle's brother's neighbor's cat's veterinarian David reads the communist manifesto in his spare time.\",\n",
       " 'He said that The Great Gatsby is the best novell ever, and I was about to throw hands.',\n",
       " 'I could not look away from this train wrck of a movie, on February 14th of all days.',\n",
       " \"The film Everything Everywhere All At Once follows Evelyn Wang, a woman drowning under the stress of her family's failing laundromat.\",\n",
       " 'I just finished reading pride and prejudice which had me HOOOKED from the beginning.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 3, 0, 3, 0, 3, 3, 2, 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set labels: 10\n",
      "Length of predictions: 10\n",
      "Test set evaluation:\n",
      "precision\trecall\tf1-score\tsupport\n",
      "sports\t0.50\t1.00\t0.67\t3.00\t\n",
      "movie\t0.75\t0.75\t0.75\t4.00\t\n",
      "book\t0.00\t0.00\t0.00\t3.00\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your code for loading the test dataset and making predictions\n",
    "\n",
    "# Convert topic labels to numerical values for classification_report\n",
    "topic_mapping = {\"sports\": 0, \"movie\": 1, \"book\": 2}\n",
    "test_df[\"topic\"] = test_df[\"topic\"].map(topic_mapping)\n",
    "\n",
    "print(\"Length of test set labels:\", len(test_df[\"topic\"]))\n",
    "print(\"Length of predictions:\", len(test_preds))\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_df[\"topic\"], test_preds, target_names=[\"sports\", \"movie\", \"book\"], output_dict=True)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Test set evaluation:\")\n",
    "print(\"\\t\".join([\"precision\", \"recall\", \"f1-score\", \"support\"]))\n",
    "for label, scores in report.items():\n",
    "    if label in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        continue\n",
    "    print(label, end=\"\\t\")\n",
    "    for metric in [\"precision\", \"recall\", \"f1-score\", \"support\"]:\n",
    "        print(f\"{scores[metric]:.2f}\", end=\"\\t\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        book       0.00      0.00      0.00         3\n",
      "       movie       0.50      0.25      0.33         4\n",
      "      sports       0.25      0.67      0.36         3\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.25      0.31      0.23        10\n",
      "weighted avg       0.28      0.30      0.24        10\n",
      "\n",
      "Counter({'sports': 8, 'movie': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=3)\n",
    "\n",
    "# Load data\n",
    "with open(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/text.txt\", \"r\") as file:\n",
    "    text_data = file.readlines()\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "encoded_data = tokenizer(text_data, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create DataLoader for prediction\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(encoded_data[\"input_ids\"]),\n",
    "    torch.tensor(encoded_data[\"attention_mask\"])\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define mapping for sentiment labels\n",
    "sentiment_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).cpu().item()\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "# Map predictions to categories\n",
    "category_map = {0: \"sports\", 1: \"movie\", 2: \"book\"}\n",
    "predicted_categories = [category_map[label] for label in predictions]\n",
    "\n",
    "# Load the true labels for evaluation\n",
    "test_data = pd.read_csv(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/sentiment-topic-test.tsv\", sep=\"\\t\")\n",
    "true_labels = test_data[\"topic\"].tolist()\n",
    "\n",
    "# Generate evaluation report\n",
    "print(classification_report(true_labels, predicted_categories))\n",
    "\n",
    "# Example of how to use Counter to see distribution of categories\n",
    "print(Counter(predicted_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        book       0.00      0.00      0.00         3\n",
      "       movie       0.00      0.00      0.00         4\n",
      "      sports       0.30      1.00      0.46         3\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.10      0.33      0.15        10\n",
      "weighted avg       0.09      0.30      0.14        10\n",
      "\n",
      "Counter({'sports': 10})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load pre-trained RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)\n",
    "\n",
    "# Load data\n",
    "with open(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/text.txt\", \"r\") as file:\n",
    "    text_data = file.readlines()\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "encoded_data = tokenizer(text_data, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create DataLoader for prediction\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(encoded_data[\"input_ids\"]),\n",
    "    torch.tensor(encoded_data[\"attention_mask\"])\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define mapping for sentiment labels\n",
    "sentiment_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).cpu().item()\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "# Map predictions to categories\n",
    "category_map = {0: \"sports\", 1: \"movie\", 2: \"book\"}\n",
    "predicted_categories = [category_map[label] for label in predictions]\n",
    "\n",
    "# Load the true labels for evaluation\n",
    "test_data = pd.read_csv(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/sentiment-topic-test.tsv\", sep=\"\\t\")\n",
    "true_labels = test_data[\"topic\"].tolist()\n",
    "\n",
    "# Generate evaluation report\n",
    "print(classification_report(true_labels, predicted_categories))\n",
    "\n",
    "# Example of how to use Counter to see distribution of categories\n",
    "print(Counter(predicted_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        book       0.00      0.00      0.00         3\n",
      "       movie       0.44      1.00      0.62         4\n",
      "      sports       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.48      0.44      0.37        10\n",
      "weighted avg       0.48      0.50      0.40        10\n",
      "\n",
      "Counter({'movie': 9, 'sports': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load pre-trained RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)\n",
    "\n",
    "# Load data\n",
    "with open(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/text.txt\", \"r\") as file:\n",
    "    text_data = file.readlines()\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "encoded_data = tokenizer(text_data, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create DataLoader for prediction\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(encoded_data[\"input_ids\"]),\n",
    "    torch.tensor(encoded_data[\"attention_mask\"])\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define mapping for sentiment labels\n",
    "sentiment_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).cpu().item()\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "# Map predictions to categories\n",
    "category_map = {0: \"sports\", 1: \"movie\", 2: \"book\"}\n",
    "predicted_categories = [category_map[label] for label in predictions]\n",
    "\n",
    "# Load the true labels for evaluation\n",
    "test_data = pd.read_csv(\"/Users/owhy/Documents/GitHub/TextMining-VU-2024/project/test_data/sentiment-topic-test.tsv\", sep=\"\\t\")\n",
    "true_labels = test_data[\"topic\"].tolist()\n",
    "\n",
    "# Generate evaluation report\n",
    "print(classification_report(true_labels, predicted_categories))\n",
    "\n",
    "# Example of how to use Counter to see distribution of categories\n",
    "print(Counter(predicted_categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
