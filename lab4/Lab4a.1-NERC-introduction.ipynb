{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4a Introduction to Named Entity Recognition and classification\n",
    "\n",
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab4a, we focus on Named Entity Recognition and Classification (NERC).\n",
    "\n",
    "We assume that you have studied the notebooks of Lab1 and Lab2 before you start this lab. Especially Lab2 is important for the assignment of this lab.\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* understand the IOB format used to format NERC data\n",
    "* load existing data sets in the IOB format to use them for training a classifier\n",
    "\n",
    "The following notebooks should be studied in this order:\n",
    "\n",
    "* Lab4a.1-NERC-introduction.ipynb (this notebook)\n",
    "* Lab4a-Assignment.ipynb\n",
    "\n",
    "We assume that you studied the previous labs and especially lab 2.\n",
    "\n",
    "In this notebook, we shortly explain some basics about NERC and the typical data formats used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NERC\n",
    "In Named Entity Recognition and Classification, the goal is to determine which noun phrases refer to named entities as well as classifying them.\n",
    "Named entities can be persons, locations, organizations, etc. (see [NLTK Chapter 7, Section 5](https://www.nltk.org/book/ch07.html) for more information on the task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not trivial to represent NERC data in a way that we can easily train NLP systems as well as evaluate them. One of the most used formats is called [Inside–outside–beginning (IOB)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). Let's look at an example from one of the most popular datasets, which is [CoNLL-2003](http://aclweb.org/anthology/W03-0419).\n",
    "```\n",
    "Germany NNP B-NP B-LOC\n",
    "'s POS B-NP O\n",
    "representative NN I-NP O\n",
    "to TO B-PP O\n",
    "the DT B-NP O\n",
    "European NNP I-NP B-ORG\n",
    "Union NNP I-NP I-ORG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation is that all information is represented at the **token-level**. For each token, e.g., *Germany*, we receive information about:\n",
    "* **the word**: e.g., *Germany*\n",
    "* **the part of speech**: e.g., *NNP* (from [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))\n",
    "* **the phrase type**: e.g., a noun phrase\n",
    "* **the NERC label**: e.g., a location (LOC).\n",
    "\n",
    "This example contains two named entities: *Germany* and *European Union*.\n",
    "\n",
    "Every first token of a named entity is prefixed with *B-*. Every token after that, e.g., *Union* in *European Union*, is prefixed with *I-*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the IOB format is at the **token-level**, which means that we also are going to train and evaluate an NLP system at the token-level! The goal will hence not be to classify *European Union* as an *Organization*, but to classify:\n",
    "* *European* as the first token of an entity that is an *Organization*\n",
    "* *Union* as a token inside of an entity that is an *Organization*\n",
    "\n",
    "Please make sure you understand the format before you proceed ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the token level representation of features and tags in the CoNLL format, we can consider NERC as a typical sequence tagging task. There is a strong dependency within a sentence across words and their tags. A sequence of words (a phrase) predicts the next seuqnece of words and the same can be said for a sequence of tags, reflecting named entity expressions or not. On top of that there can be hard constraints. We cannot have a I-tag with a different type following another I-tag in case of an IOB annotation.\n",
    "\n",
    "Machine learning approaches to NERC typically exploit these sequence dependencies by learning the most probable sequences of tags within a sentence given all possible sequences of tags, using the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm). A conditional random field classifier exploits such dependencies and allows you to define as many features as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NERC datasets\n",
    "\n",
    "Now that we've seen how to represent linguistic features, we also need to access real linguistic training data for the NERC task. In this section, we will look at large data sets that have been created by the community in which people have been annotating entities. In the assignment, you will use this data to train and test models that give a realistic performance.\n",
    "\n",
    "Here, we will load two NERC datasets and quickly inspect their contents.\n",
    "\n",
    "**Preparation** Please download the .zip file with the two datasets from [this link](https://drive.google.com/drive/folders/1LChp70lMFNL9BtNi0nVy-V-fCRkEobgr?usp=sharing)\n",
    "\n",
    "Then unpack the .zip, so that the folder `nerc_datasets` is created in the same directory as this notebook. If you want to store it elsewhere, you can do that but need to adapt the path in the calls below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CoNLL-2003\n",
    "\n",
    " One of the most popular datasets is [CoNLL-2003](http://aclweb.org/anthology/W03-0419), which was provided with the zip file you just downloaded. You can open the file \"train.txt\" in a text editor to inspect its content:\n",
    "\n",
    "````\n",
    "-DOCSTART- -X- -X- O\n",
    "\n",
    "EU NNP B-NP B-ORG\n",
    "rejects VBZ B-VP O\n",
    "German JJ B-NP B-MISC\n",
    "call NN I-NP O\n",
    "to TO B-VP O\n",
    "boycott VB I-VP O\n",
    "British JJ B-NP B-MISC\n",
    "lamb NN I-NP O\n",
    ". . O O\n",
    "\n",
    "Peter NNP B-NP B-PER\n",
    "Blackburn NNP I-NP I-PER\n",
    "\n",
    "BRUSSELS NNP B-NP B-LOC\n",
    "1996-08-22 CD I-NP O\n",
    "````\n",
    "\n",
    "It follows the IOB format with one token on a line followed by columns wit the PoS, the constituent and the IOB entity tag. You can check the \"test.txt\" file to see it has a similar format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load it using the following code snippet, which makes use of the NLTK function ConllCorpusReader to do the magic. More information on the ConllCorpusReader can be found here: https://www.nltk.org/_modules/nltk/corpus/reader/conll.html\n",
    "\n",
    "The function has three parameters:\n",
    "\n",
    "* the path to the folder where ConLL-2003 is stored (locally in my case)\n",
    "* the name of the file that will be loaded from that folder\n",
    "* labels for the columns that are expected in the input file\n",
    "\n",
    "We store the result in a variable with the name 'train' which is of the type 'nltk.corpus.reader.conll.ConllCorpusReader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/CONLL2003'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConllCorpusReader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# adapt the path to point to the local copy of the nerc_datasets folder\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mConllCorpusReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/CONLL2003\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this will load the file 'train.txt', for the exercise you also need to load 'test.xt' \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/conll.py:92\u001b[0m, in \u001b[0;36mConllCorpusReader.__init__\u001b[0;34m(self, root, fileids, columntypes, chunk_types, root_label, pos_in_tree, srl_includes_roleset, encoding, tree_class, tagset, separator)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_srl_includes_roleset \u001b[38;5;241m=\u001b[39m srl_includes_roleset\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_class \u001b[38;5;241m=\u001b[39m tree_class\n\u001b[0;32m---> 92\u001b[0m \u001b[43mCorpusReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagset \u001b[38;5;241m=\u001b[39m tagset\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m=\u001b[39m separator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/reader/api.py:80\u001b[0m, in \u001b[0;36mCorpusReader.__init__\u001b[0;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[1;32m     78\u001b[0m         root \u001b[38;5;241m=\u001b[39m ZipFilePathPointer(zipfile, zipentry)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, PathPointer):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorpusReader: expected a string or a PathPointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/CONLL2003'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "# adapt the path to point to the local copy of the nerc_datasets folder\n",
    "train = ConllCorpusReader('/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/CONLL2003',\n",
    "                          'train.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt' \n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use 'dir' to see it has many data elements that correspond to the many different features that can be found in the CoNNL data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are for now only interested in the token, the pos and the ne_label. Let's check the first one in train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU NNP B-ORG\n"
     ]
    }
   ],
   "source": [
    "for token, pos, ne_label in train.iob_words():\n",
    "    print(token, pos, ne_label) # please represent this information using a dictionary for the feature representation\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example iterate through this data, and make a list of the tokens as inputs, and of the `ne_label` values as desirable outputs. \n",
    "The input tokens could for example be looked up in a word embeddings dictionary (see Lab2 for details on word embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/piek/Desktop/ONDERWIJS/data/model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully loaded our data. Let's see how many tokens/labels we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ten labels = ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print('Last ten labels =', labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we should have the same size of input_vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(input_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we could easily train a model on this data as shown in above by combining the input vectors with the labels in a fit function. \n",
    "You will see it takes a lot longer to train the classifier with this  data set that has over 200K instances.\n",
    "On my machine it took about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf.fit(input_vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to apply this classifier to a data set for testing, you need to apply the same vectorization procedure as you have followed for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you apply a classifier to a data set, it is important to know the data set and especially the statistics about how the labels are distributed. In other words, how often do tokens in the data set belong a human annotated data set?\n",
    "\n",
    "This tells you how frequent or rare certain data categories are and how challenging it is for a system to learn and predict each category.\n",
    "\n",
    "Because we have created a list of labels from our data, we can use a simple Python function *Counter* to get the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly shows that most tokens get the label *O* and the actual enity tokens range between 1155 and 7140."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Kaggle\n",
    "[*Kaggle*](https://www.kaggle.com/docs) is an open source platform for sharing data and competitions. It has over 1000's of datasets and  frequently releases new data and challenges. We are going to have a quick look at the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) that they provided and which was also provided in the zip file you downloaded as a so-called CSV file: ner.csv and ner_v2.csv. CSV stands for comma-separated-values and it is a commonly used format to exchange e.g. Excell or spreadsheet data as text files. Instances of data are represented on separate lines followed by values separated by commas. Another format is tab-separated-values or TSV, in which case tabs are used as in the CoNLL formats. Very often people store TSV formats in files with the extension \".csv\", so it is always good practice to check the actual content to see what is used as a separator. The first line of a CSV or TSV file is usually the header that labels the different columns. \n",
    "\n",
    "The [*pandas*](https://pandas.pydata.org) package is a powerful package to handle data in various formats. You can check the website for details and documentation. Here we are going to use it to inspect the data.\n",
    "\n",
    "Let's load the data in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# Adapt the path to point to your local copy of the nerc_datasets\n",
    "path = '/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following output after running the above code cell:\n",
    "```\n",
    "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
    "```\n",
    "You can ignore this.\n",
    "\n",
    "**pandas.read_csv** will load the csv file into a [pandas DataFrame](https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect which columns are in the csv file by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n",
       "       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n",
       "       'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos',\n",
       "       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n",
       "       'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape',\n",
       "       'word', 'tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can seen that a wide range of features is given for each token. [Here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), you can read what each column represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loop can loop through the dataset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "id                             0\n",
      "lemma                   thousand\n",
      "next-lemma                    of\n",
      "next-next-lemma         demonstr\n",
      "next-next-pos                NNS\n",
      "next-next-shape        lowercase\n",
      "next-next-word     demonstrators\n",
      "next-pos                      IN\n",
      "next-shape             lowercase\n",
      "next-word                     of\n",
      "pos                          NNS\n",
      "prev-iob              __START1__\n",
      "prev-lemma            __start1__\n",
      "prev-pos              __START1__\n",
      "prev-prev-iob         __START2__\n",
      "prev-prev-lemma       __start2__\n",
      "prev-prev-pos         __START2__\n",
      "prev-prev-shape         wildcard\n",
      "prev-prev-word        __START2__\n",
      "prev-shape              wildcard\n",
      "prev-word             __START1__\n",
      "sentence_idx                   1\n",
      "shape                capitalized\n",
      "word                   Thousands\n",
      "tag                            O\n",
      "Name: 0, dtype: object\n",
      "NERC label O\n"
     ]
    }
   ],
   "source": [
    "for index, instance in kaggle_dataset.iterrows():\n",
    "    print()\n",
    "    print(index)\n",
    "    print(instance) # you can access information by using instance['A COLUMN NAME'] which you can use to convert to a dictionary needed for the feature representation.\n",
    "    print('NERC label', instance['tag'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each token has many different features that people have considered useful for trhe task of NERC. In addition to the usual suspects that we saw before, each token also has features indicating previous and next words and their PoS, but als the shape of the word (upper and lower case patterns), and even the previous IOB tags.\n",
    "\n",
    "We could use all these features as inputs in a machine learning model with our DictVectorizer, or by transforming them using embeddings if the values are words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
