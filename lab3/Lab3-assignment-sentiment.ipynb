{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER is based on lexicons of sentiment-related words. Each of the words is rated as positive or negative, but they are also rated based on the degree in which they are positive or negative. This means that more positive words get a higher positive rating, and more negative words get a more negative rating. \n",
    "The first input sentence has a compound value of 0.6 which is a quite strong positive value, which is accurate for a sentence indicating that something is loved. Punctuation marks are taken into account by VADER, since this tool is often used in social media content where punctuation is often used to express sentiment. This explains why the compound value of the third input sentence is higher compared to the first, while the words are the same.\n",
    "VADER takes the context of the entire sentence into account, explaining the 0.0 positive value for the second sentence, even though it contains the word 'love'. It therefore produces a correct negative compound value of -0.5\n",
    "For the fourth input sentence, the word ruins has a negative connotation when relating to houses, therefore the negative compound value of -0.4 is correct. The fifth input sentence also contains the negatively loaded word 'ruins', but also the word 'not' which indicates that this sentence is positively loaded on the contrary. This also explains why the negative value for this input sentence is 0.0\n",
    "The output for the sixth input sentence is incorrect. This sentence is neutral and does not convey any emotion or sentiment, yet the compound value is a negative value of -0.4. A possible explanation might be that the word 'lies' is interpreted as the wrong verb.\n",
    "The compound value for the seventh input sentence is accurate because the sentence is neutral and does not convey any emotion as well, just like the previous input sentence. However, we can observe that the compound values between these two are quite different. An explanation might be that the word 'like' causes the positive value to be higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import json\n",
    "from nltk.sentiment import vader\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # 'en_core_web_sm'\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "def run_vader(\n",
    "    textual_unit, lemmatize=False, parts_of_speech_to_consider=None, verbose=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "\n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "\n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == \"-PRON-\":\n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(\" \".join(input_to_vader))\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print(\"INPUT SENTENCE\", sent)\n",
    "        print(\"INPUT TO VADER\", input_to_vader)\n",
    "        print(\"VADER OUTPUT\", scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.61      0.67        18\n",
      "     neutral       0.47      0.47      0.47        15\n",
      "    positive       0.55      0.65      0.59        17\n",
      "\n",
      "    accuracy                           0.58        50\n",
      "   macro avg       0.58      0.57      0.58        50\n",
      "weighted avg       0.59      0.58      0.58        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"my_tweets.json\", \"r\") as f:\n",
    "    my_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)  # run vader\n",
    "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1: Quantitative Analysis\n",
    "\n",
    "In the classification report table we can metrics such as precision, recall, f1-score, and support. \n",
    "\n",
    "First, I would like to explain each metric and why they are being explored. Precision measures accuracy of positive predictions among all positive instances (73% of negative tweets were actually labeled as negative by VADER). Recall demonstrates the ability of the system to identify all correct labels in the sample. (61% of all existing negative tweets were correctly labeled as negative). F1 score combines the scores of precision and recall, higher f1 indicates better performance. Support shows how many times different classes occur. Accuracy shows the proportion of correctly predicted sentiments in the whole population together. Lastly, we have macro-average and weighted-average. These two measures are ways to play around with class imbalances and whether differences in class count have an impact on the metrics above. Macro-average strips class imbalance from the metrics, precision, recall, and f1 is calculated for all classes individually and an average is taken from all. For weighted-average, the number of instances in each class is taken into account so, unlike macro-average, it doesn't consider all classes as equal as it takes into account that classes with more instances are more important. \n",
    "\n",
    "We can see that all these metrics are useful tools for understanding the results. In conclusion, precision is an important metric to explore as it demonstrates that the model identifies negative tweets better compared to neutral or positive ones (all metrics for that category are higher). Furthermore, the accuracy shows 58% as 58% of tweets were correctly labeled by the model. F1 is another critical metric as it strikes a balance between recall and precision, thus, looking at the f1 score gives us a holistic view on the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Rival dad next door and his family are moving today. So I'm sitting out front judging his u-haul tetris stacking abilities and giving a disapproving groan when he does something wrong.\n",
      "Predicted Label: positive\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: Avoid eye contact, he may ask you to help!\n",
      "Predicted Label: positive\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: Great way to kill a little extra time!\n",
      "Predicted Label: negative\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: “Only stacking two boxes on the dolly at once I see, huh Terry? Hm. Guess that’s one way to go.”\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: Whatever will you do for entertainment now? How else will you assert your superior masculinity?\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: One time, my uncle's VERY inexperienced neighbour was removing a fair-sized tree by himself. He watched and chuckled he's going to kill himself...' the tree came down - right onto my uncle's fence.\n",
      "Predicted Label: negative\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: “My body is a wonderland” I whisper as I pluck the last of my chin hairs\n",
      "Predicted Label: neutral\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: A leap of faith is scheduling a kids birthday party in the middle of winter and expecting everyone to be healthy when the day arrives\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: Lol I saw this too and died. What are you gonna do, make the baby stay alone in her room all day????\n",
      "Predicted Label: negative\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: It does make me wonder how that dynamic shifts as the kids hit their teenage years. Most teens I know treat their bedrooms like personal forts of solitude.\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: I’m here to demand an update in about 12-14 years from now\n",
      "Predicted Label: negative\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: I have 4 kids and I don’t care what room they’re in\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: There really should be an application process to procreate\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: You gotta wonder sometimes if dad jokes are actually just mom jokes repeated by a guy in a louder voice\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: bet he's never held a power tool in his life. poser.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: lil bro never lacking\n",
      "Predicted Label: neutral\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: You got soft paws boy. Soft paws\n",
      "Predicted Label: neutral\n",
      "Actual Label: positive\n",
      "\n",
      "Tweet: Ur on thin ice missy\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Tweet: Got a feeling this one is NOT forklift certified\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: I am confused about who is lucky here\n",
      "Predicted Label: positive\n",
      "Actual Label: neutral\n",
      "\n",
      "Tweet: simping is cringey.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "misclassified_tweets = []\n",
    "\n",
    "for tweet, predicted_label, actual_label in zip(tweets, all_vader_output, gold):\n",
    "    if predicted_label != actual_label:\n",
    "        misclassified_tweets.append((tweet, predicted_label, actual_label))\n",
    "\n",
    "# Print misclassified tweets\n",
    "for tweet, predicted_label, actual_label in misclassified_tweets:\n",
    "    print(\"Tweet:\", tweet)\n",
    "    print(\"Predicted Label:\", predicted_label)\n",
    "    print(\"Actual Label:\", actual_label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2: Error Analysis\n",
    "\n",
    "1. Misclassified tweets were identified in the code above with the tweet, the predicted label and the actual label. Now we take 10 from each category to analyse why they were misclassified.\n",
    "2. Select 10 misclassified tweets for each sentiment category --> there are 7 negative, 8 neutral, and 6 positive tweets that were wrongly classified.\n",
    "3. Analyze the content of these tweets to understand why they were misclassified (refer to the VADER lexicon and rules to identify potential reasons for misclassification) such as ambiguous language, sarcasm, negations, or emoticons.\n",
    "\n",
    "As the tweets are visualized above, I will not analyse each tweet individually as that would be repetitive and not time efficient. I will explore the main findings and reasons behind why they were misclassified in the section below.\n",
    "\n",
    "First, when it comes to the negative tweets that were misclassified, 2 were classified as positive and 5 as neutral. Meaning that the model had a propensity to classify negative tweets as neutral tweets. Looking deeper we can see that tweets such as \"A leap of faith is scheduling a kids birthday party in the middle of winter...\" have a positive conotation due to the phrase \"leap of faith\" but in actuality, this is being said in a sarcastic way, resulting in an originally negative tweet. Lastly, there is the tweet \"Rival dad next door and his family are moving today...\", this is marked as positive but it misses the fact that the writer judges the dad in a negative manner. \n",
    "\n",
    "Second, out of 8 neutral tweets that were misclassified, 7 were classified as positive and 1 was negative. Here we can see that neutral tweets were seen by the model as more positive. Looking deeper at the specific tweets, we can see that \"Whatever will you do for entertainment now?...\" has been labeled as positive but in actuality doesn't express positivity but rather is a neutral rhetorical question, it doesn't convey positive curiosity. The tweet \"I’m here to demand an update in about 12-14 years from now\" is labeled as negative but in actuality conveys a neutral, maybe even a positive playful tone. However, VADER interpreted the humorous tone as one that is negative and overlooked the sentiment expressed.\n",
    "\n",
    "Lastly, out of the 6 positive tweets misclassified, 3 were classified as negative and 3 as neutral. This is interesting as it seems that the model had serious problems identifying positive sentiment as it misclassified 50% of the misclassified cases as negative. Looking deeper we can see that in the tweet: \"Great way to kill a little extra time!\", VADER possibly misinterpreted the phrase \"kill a little extra time\" as negative due to the word \"kill,\" overlooking the overall positive sentiment of the tweet. Furthermore, in the tweet: \"Lol I saw this too and died. What are you gonna do, make the baby stay alone in her room all day????\", VADER predicted it to be negative due to heavy language used such as \"die\" but did not take into account internet language such as \"lol\" and the sarcastic tone used in the tweet.\n",
    "\n",
    "It is evident that these misclassifications occurred due to  VADER's lexicon; its rules may not fully capture the nuances of everyday language, such as sarcasm, humor, or rhetorical devices, leading to errors in sentiment classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "\n",
    "[1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "\n",
    "[3 points] b. Compare the scores and explain what they tell you.\n",
    "- Does lemmatisation help? Explain why or why not.\n",
    "- Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are choosing the subset of negative tweets, as negative labels were the ones most accurately predicted by the model. First, we need to convert the txt files in the negative folder into .json for easier implementation of VADER. After this implementation, we apply for all settings of the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def compile_tweets_to_json(main_folder):\n",
    "    data = {}\n",
    "    tweet_count = 0\n",
    "\n",
    "    # Loop through each folder (positive, negative, neutral) in the main folder\n",
    "    for sentiment_label in os.listdir(main_folder):\n",
    "        sentiment_folder = os.path.join(main_folder, sentiment_label)\n",
    "\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(sentiment_folder):\n",
    "            # Loop through each txt file in the sentiment folder\n",
    "            for file_name in os.listdir(sentiment_folder):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    tweet_count += 1\n",
    "                    file_path = os.path.join(sentiment_folder, file_name)\n",
    "\n",
    "                    # Read the content of the txt file\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        tweet_text = file.read().strip()\n",
    "\n",
    "                    # Add tweet data to the dictionary\n",
    "                    data[str(tweet_count)] = {\n",
    "                        \"sentiment_label\": sentiment_label,\n",
    "                        \"text_of_tweet\": tweet_text,\n",
    "                        \"tweet_url\": \"\",  # You can add tweet URLs if you have them\n",
    "                    }\n",
    "\n",
    "    # Shuffle the order of tweets\n",
    "    shuffled_data = {k: data[k] for k in random.sample(list(data.keys()), len(data))}\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(\"compiled_tweets.json\", \"w\") as json_file:\n",
    "        json.dump(shuffled_data, json_file, indent=4)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "main_folder = \"airlinetweets\"\n",
    "compile_tweets_to_json(main_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all negative tweets are converted into the appropriate format, we apply VADER in different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER (as it is) on the set of airline tweets\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(the_tweet)  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets after having lemmatized the text\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(the_tweet, lemmatize=True)  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only adjectives\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        parts_of_speech_to_consider={\"ADJ\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        lemmatize=True,\n",
    "        parts_of_speech_to_consider={\"ADJ\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.24      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.34      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only nouns\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        parts_of_speech_to_consider={\"NOUN\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.16      0.26      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.52      0.33      0.40      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.39      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        lemmatize=True,\n",
    "        parts_of_speech_to_consider={\"NOUN\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.29      0.42      1750\n",
      "     neutral       0.38      0.81      0.52      1515\n",
      "    positive       0.57      0.34      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only verbs\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        parts_of_speech_to_consider={\"VERB\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.30      0.42      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.56      0.48      0.46      4755\n",
      "weighted avg       0.57      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "tweets = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "with open(\"compiled_tweets.json\", \"r\") as f:\n",
    "    airline_tweets = json.load(f)\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True\n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in airline_tweets.items():\n",
    "    the_tweet = tweet_info[\"text_of_tweet\"]\n",
    "    vader_output = run_vader(\n",
    "        the_tweet,\n",
    "        lemmatize=True,\n",
    "        parts_of_speech_to_consider={\"VERB\"},\n",
    "    )  # run vader\n",
    "    vader_label = vader_output_to_label(\n",
    "        vader_output\n",
    "    )  # convert vader output to category\n",
    "\n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info[\"sentiment_label\"])\n",
    "\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gold, all_vader_output)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3 points] b. Compare the scores and explain what they tell you.\n",
    "\n",
    "- *Does lemmatisation help? Explain why or why not.*\n",
    "\n",
    "Comparing them, we can see that there is no significant difference between the performance of models with and without lemmatization. Models with and without perform similarly across all sentiment labels. We can see that the normalization method can help improve the performance in general but in this specific case, it demonstrates limited effectiveness. VADER is specifically designed for social media text such as in this case with many informal words which reduces its effectiveness. \n",
    "\n",
    "- *Are all parts of speech equally important for sentiment analysis? Explain why or why not.*\n",
    "\n",
    "Although lemmatisation had limited effect, we can see that different parts of speech do have varying effects. Models trained on ADJectives show better performance compared to models trained on verbs or nouns. Adjectives are usually more informative of the sentiment as they are used to describe attributes or the qualities of nouns. This is the main reason why adjectives would be easier for the model to identify.\n",
    "\n",
    "Second, nouns are lower on the performance scale than both verbs and adjectives and don't convey sentiment as effectively as adjectives. Nouns are more neutral and don't often indicate sentiment. \n",
    "\n",
    "Third, models trained on verbs are lower on performance compared to adjectives but perform very similarly to nouns. Describing states and actions could indirectly convey sentiment depending on the context but they are still not as direct as adjectives and thus, demonstrate lower perfomance which was expected.\n",
    "\n",
    "In conclusion, it is always best to consider a combination of different parts of speech, providing a more comprehensive understanding of sentiment in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_files in module sklearn.datasets._base:\n",
      "\n",
      "load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0, allowed_extensions=None)\n",
      "    Load text files with categories as subfolder names.\n",
      "\n",
      "    Individual samples are assumed to be files stored a two levels folder\n",
      "    structure such as the following:\n",
      "\n",
      "        container_folder/\n",
      "            category_1_folder/\n",
      "                file_1.txt\n",
      "                file_2.txt\n",
      "                ...\n",
      "                file_42.txt\n",
      "            category_2_folder/\n",
      "                file_43.txt\n",
      "                file_44.txt\n",
      "                ...\n",
      "\n",
      "    The folder names are used as supervised signal label names. The individual\n",
      "    file names are not important.\n",
      "\n",
      "    This function does not try to extract features into a numpy array or scipy\n",
      "    sparse matrix. In addition, if load_content is false it does not try to\n",
      "    load the files in memory.\n",
      "\n",
      "    To use text files in a scikit-learn classification or clustering algorithm,\n",
      "    you will need to use the :mod:`~sklearn.feature_extraction.text` module to\n",
      "    build a feature extraction transformer that suits your problem.\n",
      "\n",
      "    If you set load_content=True, you should also specify the encoding of the\n",
      "    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "    will be the correct encoding. If you leave encoding equal to None, then the\n",
      "    content will be made of bytes instead of Unicode, and you will not be able\n",
      "    to use most functions in :mod:`~sklearn.feature_extraction.text`.\n",
      "\n",
      "    Similar feature extractors should be built for other kind of unstructured\n",
      "    data input such as images, audio, video, ...\n",
      "\n",
      "    If you want files with a specific file extension (e.g. `.txt`) then you\n",
      "    can pass a list of those file extensions to `allowed_extensions`.\n",
      "\n",
      "    Read more in the :ref:`User Guide <datasets>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    container_path : str\n",
      "        Path to the main folder holding one subfolder per category.\n",
      "\n",
      "    description : str, default=None\n",
      "        A paragraph describing the characteristic of the dataset: its source,\n",
      "        reference, etc.\n",
      "\n",
      "    categories : list of str, default=None\n",
      "        If None (default), load all the categories. If not None, list of\n",
      "        category names to load (other categories ignored).\n",
      "\n",
      "    load_content : bool, default=True\n",
      "        Whether to load or not the content of the different files. If true a\n",
      "        'data' attribute containing the text information is present in the data\n",
      "        structure returned. If not, a filenames attribute gives the path to the\n",
      "        files.\n",
      "\n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "\n",
      "    encoding : str, default=None\n",
      "        If None, do not try to decode the content of the files (e.g. for images\n",
      "        or other non-text content). If not None, encoding to use to decode text\n",
      "        files to Unicode if load_content is True.\n",
      "\n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. Passed as keyword\n",
      "        argument 'errors' to bytes.decode.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=0\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    allowed_extensions : list of str, default=None\n",
      "        List of desired file extensions to filter the files to be loaded.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "\n",
      "        data : list of str\n",
      "            Only present when `load_content=True`.\n",
      "            The raw text data to learn.\n",
      "        target : ndarray\n",
      "            The target labels (integer index).\n",
      "        target_names : list\n",
      "            The names of target classes.\n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        filenames: ndarray\n",
      "            The filenames holding the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/owhy/Desktop/github/TextMining-VU-2024/airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath(\"airlinetweets\")\n",
    "print(\"path:\", airline_tweets_folder)\n",
    "print(\"this will print True if the folder exists:\", airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/owhy/Desktop/github/TextMining-VU-2024/airlinetweets'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data.\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4755"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
